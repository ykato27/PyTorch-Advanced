{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UbJrC7Fq43T"
   },
   "source": [
    "「 Autograd（自動微分）」\n",
    "===============================================================\n",
    "【原題】Autograd: Automatic Differentiation\n",
    "\n",
    "【原著】[Soumith Chintala](http://soumith.ch/)\n",
    "\n",
    "【元URL】https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n",
    "【翻訳】電通国際情報サービスISID AIトランスフォーメーションセンター　徳原　光\n",
    "\n",
    "【日付】2020年10月27日\n",
    "\n",
    "【チュトーリアル概要】\n",
    "\n",
    "PyTorchによるニューラルネットワークの学習において、重要な概念でありパッケージでもあるautogradの機能、そしてその動作内容について解説します。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-Amyp7mS-Uc"
   },
   "source": [
    "Autograd: 自動微分\n",
    "=================\n",
    "PyTorchによるニューラルネットワーク構築の土台となっているのが、autograd（自動微分）パッケージです。\n",
    "\n",
    "このパッケージの概要をざっくりと確認し、本チュートリアルシリーズで初めてとなるニューラルネットワークの訓練を体験しましょう。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqM3lrlxsJVD"
   },
   "source": [
    "autogradパッケージはTensorの操作に対する自動微分機能を提供します。\n",
    "\n",
    "Tensor操作に基づく自動微分はdefine-by-runフレームワークであり、ユーザーが実行したコードに対応して誤差逆伝搬が定義され、すべてのイテレーションの計算で異なる結果を生み出します。\n",
    "\n",
    "ここからは、いくつかの実例を通してこの機能を簡単に見ていきましょう。\n",
    "\n",
    "<br>\n",
    "\n",
    "（日本語訳注：define-by-runはデータをニューラルネットワークに流しながら、モデルの構築を行っていく手法を指します。一方でdefine-and-runは先に誤差逆伝搬の形を実行する前に構築します。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJRlOUfrPezQ"
   },
   "source": [
    "\n",
    "テンソル（Tensor）\n",
    "==================\n",
    "``torch.Tensor``はパッケージの中心的なクラスです。\n",
    "\n",
    "``.requires_grad``属性が``True``に設定された場合、autogradパッケージによってすべての操作が追跡され、演算が終了した際は``.backward()``を呼び出すことで、すべての操作に対する勾配が自動的に計算されます。\n",
    "\n",
    "このTensorに対する勾配は``.grad``属性に蓄積されていきます。\n",
    "\n",
    "追跡履歴からTensorを切り離して、追跡を停止する場合は、 ``.detach()``を呼び出します。\n",
    "\n",
    "これにより、その後の演算ではこのこのTensorは追跡されないように設定可能です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHM2R7Btsda3"
   },
   "source": [
    "``with torch.no_grad():``でコードをブロックにまとめることで、追跡履歴（とメモリの利用）を省略することもできます。\n",
    "\n",
    "これはモデルを評価する際、``requires_grad=True``により学習可能なパラメータを持っているが、勾配の計算は必要ない場合に特に有効です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0NiMGxlOe4x"
   },
   "source": [
    "そしてもう一つ、自動微分の実行に非常に重要なクラスに``Function``があります。\n",
    "\n",
    "``Tensor``と``Function``は 相互に接続し、非巡回グラフで完全な計算履歴を記録しています。\n",
    "\n",
    "各Tensorは、そのTensorを作成した``Function`` を参照する``.grad_fn``属性を持ちます（ユーザーが直接定義したテンソルの場合は``grad_fn is None``となります）。\n",
    "\n",
    "導関数を算出する場合は、``Tensor``が持つ関数``.backward()``を呼び出します。\n",
    "\n",
    "``Tensor``がスカラー（すなわち、要素数が1つだけ）の場合、``.backward()``に引数を指定する必要はありません。しかし、テンソルが複数要素を持つ場合は、Tensorと同じ大きさのTensorを``gradient``の引数に指定する必要があります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KEFlGoOZAMR7"
   },
   "outputs": [],
   "source": [
    " %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IUxlqaEXPezR"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugx1PnHBPezU"
   },
   "source": [
    "Tensorを作成し、``requires_grad=True``と指定することによって演算を追跡してみましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SmN6W3fVPezU",
    "outputId": "96c34493-f1b4-444e-89d3-f888367f2874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l427YOrPezY"
   },
   "source": [
    "Tensorの計算を実行：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdDnSkYDPezY",
    "outputId": "3711158d-f375-4889-d28f-64af4cb29f01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk9ssq5QPeza"
   },
   "source": [
    "``y`` は計算結果であり、 計算履歴として``grad_fn``を持っています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpUz2YwFPezb",
    "outputId": "b41b87f1-5998-4ea5-8504-a19463234a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f7f3e698518>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7-fxhfEPezd"
   },
   "source": [
    "さらに、``y``を用いた計算を実行します：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjx0OqiHPeze",
    "outputId": "d0d70649-ce1b-4375-9c8c-a89ed33dd4e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5_AQV-5Pezh"
   },
   "source": [
    "``.requires_grad_( ... )`` によって既存のTensorの ``requires_grad``フラグを変更することができます。\n",
    "\n",
    "テンソルの作成時に引数で何も指定していない場合は、``requires_grad``はデフォルト値として ``False`` に設定されています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJ2GLGJgPezi",
    "outputId": "344b79f1-07af-4e9f-b551-25df0c360cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7f7ef1826320>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqq783Z1Pezl"
   },
   "source": [
    "勾配（Gradients）\n",
    "==================\n",
    "では、誤差逆伝搬を実行してみましょう。\n",
    "\n",
    "変数``out``はスカラーの値を持っているため, ``out.backward()`` は``out.backward(torch.tensor(1.))``と同じ結果になります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "V0V_zGPzPezl"
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aiM8cjlDs5Df",
    "outputId": "b133f08c-0b22-4c68-ee19-ceec12c62e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPcj6hy6Pezo"
   },
   "source": [
    "勾配 d(out)/dxを表示。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3VI_KMyPezp",
    "outputId": "4970a569-d24e-4d12-ee87-4d837d6d06ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWCobsqVPezr"
   },
   "source": [
    "結果としてすべての要素が ``4.5``の行列を得たのではないでしょうか。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwoB0GDyQum3"
   },
   "source": [
    "この出力テンソルを“$o$”と記載します。\n",
    "テンソル“$o$”を計算すると、\n",
    "\n",
    "$o = \\frac{1}{4}\\sum_i z_i$\n",
    "\n",
    "$z_i = 3(x_i+2)^2$ \n",
    "\n",
    "そして、\n",
    "\n",
    " $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
    "\n",
    "なので、\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$\n",
    "\n",
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrBVkBEdPezr"
   },
   "source": [
    "\n",
    "数学的には、ベクトル関数 $\\vec{y}=f(\\vec{x})$において、 $\\vec{x}$に関する$\\vec{y}$ の勾配はヤコビアンと呼ばれています。\n",
    "\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mITIyYK1uNSJ"
   },
   "source": [
    "\n",
    "一般的に、 ``torch.autograd`` はベクトルのヤコビアンの積を算出する計算エンジンになります。 \n",
    "\n",
    "\n",
    "\n",
    "これは、$v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$というベクトルに対して、行列積$v^{T}\\cdot J$を計算することを意味します。\n",
    "\n",
    " もし、 $v$ がスカラー関数の勾配 $l=g\\left(\\vec{y}\\right)$であった場合、\n",
    "$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$と表されます。\n",
    "\n",
    "そして連鎖律により、ベクトルのヤコビアンの積は$\\vec{x}$に関する$l$の勾配となるのです。\n",
    "\n",
    "\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial y_{m}}\n",
    "   \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5Aq5nvp_C31"
   },
   "source": [
    "( $v^{T}\\cdot J$ は転置の公式から、 $J^{T}\\cdot v$を計算して得られる列ベクトル、と同じ成分の行ベクトルを与えることに注意してください)\n",
    "\n",
    "このベクトルのヤコビアンの積の性質は、スカラ量ではない出力を持つモデルに対して、外部から異なる勾配を追加して計算する際に、非常に有効に利用できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11kqmutDBxpK"
   },
   "source": [
    "ベクトルのヤコビアンの積の例を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jObNZxwcPezs",
    "outputId": "2e70e5d4-c821-4378-98cb-5f7fbd5de076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-100.0453, -496.9997,  898.9301], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6uh8nEBWYRv"
   },
   "source": [
    "（日本語訳注：関数norm()はノルム≒距離を与えます、引数なしのnorm()は2乗ノルムです。\n",
    "各要素を2乗して足し算し、そのルートを計算します）\n",
    "\n",
    "よって上記では、乱数で発生させたxを2倍、4倍、8倍、・・・とyの3要素の2乗和の平均のルートが1000を超えるまで増加させています。\n",
    "\n",
    "以下の計算結果を参照）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcQGt4PnXPCN",
    "outputId": "5e61471e-ba3a-456a-f302-3ab99742535a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1032.0334, grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 日本語訳注\n",
    "torch.sqrt(y[0]*y[0] + y[1]*y[1] + y[2]*y[2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mqnhD_ePezu"
   },
   "source": [
    "この場合、 ``y`` はスカラ量ではありません。 ``torch.autograd``\n",
    "では直接ヤコビアンの全要素を計算することはできませんが、ベクトルとヤコビアンの積を計算するだけの場合には、簡単に引数として``backward``にベクトルを与えることで勾配が算出できます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQA2eq51Pezv",
    "outputId": "72bdf940-7fb9-4854-b417-e8a1853edef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFELk6eua6Sp",
    "outputId": "bd7bd41d-ef1d-4bd8-8163-bc8f145afe58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x： tensor([-0.1954, -0.9707,  1.7557], requires_grad=True)\n",
      "y： tensor([-100.0453, -496.9997,  898.9301], grad_fn=<MulBackward0>)\n",
      "倍数： tensor([512., 512., 512.], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 日本語訳注\n",
    "print(\"x：\", x)\n",
    "print(\"y：\", y)\n",
    "scale = y/x\n",
    "print(\"倍数：\", scale)\n",
    "\n",
    "# このセルの出力から何倍して、yを求めたのか分かります。倍数:　の出力部分です。\n",
    "# このセルでは変数sclaeで表しています。 \n",
    "# y = scale * x なので、yのxに関する勾配は scaleです。\n",
    "# yの勾配をvに対して計算した結果、xに対して溜まる勾配値がx.gradです。\n",
    "# その値x.grad（上記のセルの出力結果）は、\n",
    "# scaleにv＝[0.1, 1, 0.0001]がかけ算された値となっているはずです。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu8ZOwFOPezy"
   },
   "source": [
    "``with torch.no_grad():``でコードをまとめることで、``.requires_grad=True``となっているTensorの追跡履歴からautogradを停止することができます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ph4dTCdjPez0",
    "outputId": "775a94ff-eb0e-4c08-f7b9-fa7b85149b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjNMWxnxFehy"
   },
   "source": [
    "要素が同じTensorを作成した際も、勾配が必要ない場合は``.detach()``を用いることも可能です。\n",
    "\n",
    "（日本語訳注：以下のセルの.eq()はTensorとして値が同じであればTrueを返します。いまは、xとyの要素は3つあるので、その3つに対して求めた結果を.all()でまとめて求めています　[詳細](https://pytorch.org/docs/stable/generated/torch.eq.html)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzVdQiRZPez2",
    "outputId": "e9eb8d6a-a78e-45de-9928-8e2fd27d4d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJADPZLdPez5"
   },
   "source": [
    "\n",
    "**補足:**\n",
    "\n",
    "``autograd.Function`` のドキュメントは\n",
    "https://pytorch.org/docs/stable/autograd.html#function\n",
    "を参照してください。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_2_autograd_tutorial_jp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
