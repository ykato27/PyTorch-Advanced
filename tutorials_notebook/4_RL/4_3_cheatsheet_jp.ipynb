{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_3_cheatsheet_jp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXMgDjEcZ47S"
      },
      "source": [
        "# 強化学習チートシート \n",
        "\n",
        "【原題】RL CHEAT SHEET\n",
        "\n",
        "【元URL】https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N\n",
        "\n",
        "【翻訳】電通国際情報サービスISID AIトランスフォーメーションセンター　御手洗 拓真\n",
        "\n",
        "【日付】2020年12月28日\n",
        "\n",
        "【チュトーリアル概要】\n",
        "\n",
        "強化学習の各種概念、専門用語について、解説します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMfuO883blEq"
      },
      "source": [
        "## 基本的な考え方 \n",
        "\n",
        "\n",
        "![pic](https://drive.google.com/uc?id=1S7fdgKYw-1R1zwyUVOAC6qdlFA9_an2M)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chyu7AVObwWP"
      },
      "source": [
        "### Action\n",
        "$$\n",
        "a \\in {\\rm I\\!R}^{d_a}\n",
        "$$\n",
        "\n",
        "強化学習の枠組みでは、**行動（Action）**は、エージェントがコントロールできる唯一の要素です。\n",
        "\n",
        "エージェントは、環境から返される「報酬」や「状態」を無視してランダムに行動するか、もしくは、次のステップからの報酬を最大化するように≒貪欲（greedy）に行動を選択することができます。\n",
        "\n",
        "すなわち行動とは、「エージェントが環境に対しどのような働きかけをするか」ということを表しています。\n",
        "\n",
        "行動は、実数もしくはベクトルで表現されます。\n",
        "\n",
        "また、エージェントが取り得る全ての行動の集合を**行動空間**と呼びます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36WmEZ-8bn9M"
      },
      "source": [
        "### 状態（State）\n",
        "$$\n",
        "s \\in {\\rm I\\!R}^{d_s}\n",
        "$$\n",
        "**状態（State）**とは、現在の環境を表す情報です。\n",
        "\n",
        "エージェントは状態を直接コントールすることはできません（行動を介して状態を変化させることになります）。\n",
        "\n",
        "行動と同様に、状態は実数値かベクトルで表され、環境がなり得る全ての状態の集合を**状態空間と**呼びます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm0EqRQqbo09"
      },
      "source": [
        "### 報酬（Reward）\n",
        "$$\n",
        "r \\in {\\rm I\\!R}\n",
        "$$\n",
        "**報酬（Reward）**は、環境からエージェントに対するフィードバックです。\n",
        "\n",
        "この報酬によってエージェントは学習し、行動選択の考え方を変えていきます。\n",
        "\n",
        "報酬をイメージする際には、「教師あり学習における学習データ」に相当するものとして考えると分かりやすいかもしれません。\n",
        "\n",
        "また、行動や状態とは異なり、報酬は常に単一の実数値で表現されます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP3w96OO2h4e"
      },
      "source": [
        "基本的な関係 \r\n",
        "------------\r\n",
        "\r\n",
        "$$a_{t} \\sim \\pi(s_{t})$$\r\n",
        "\r\n",
        "さて、それではエージェントの行動は何によって決まるのでしょうか？\r\n",
        "\r\n",
        "エージェントは、環境を観測した結果（＝状態）に基づいて行動を決めています。\r\n",
        "\r\n",
        "報酬は将来の行動を変えはしますが、現在の行動には影響しません。\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP2sthAWm98L"
      },
      "source": [
        "\r\n",
        "\r\n",
        "また、上記の数式における関数$\\pi$は**方策（Policy）**と呼ばれます。\r\n",
        "\r\n",
        "*方策*は、エージェントが環境に対して取る行動を決めるための戦術です。\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "上記の数式では$=$ではなく$\\sim$を利用していますが、これは*方策*が（行動そのものではなく）確率分布を返すためです。\r\n",
        "\r\n",
        "例えば、$\\epsilon$-greedy法に基づいた方策では、エージェントは$\\epsilon$という確率でランダムに行動を選択することになります。\r\n",
        "\r\n",
        "RLの枠組みにおける各プロセス（行動の選択など）は決定論的ではなく、動的なものとなっているのです。\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75BArUsA4Xo_"
      },
      "source": [
        "-----\r\n",
        "\r\n",
        "$$s_{t + 1} \\sim f(s_{t},a_{t})$$\r\n",
        "\r\n",
        "ここでは添字$t$によってタイムステップ（時間ステップ）を表しています。\r\n",
        "\r\n",
        "環境の次の時間ステップでの状態$s_{t + 1}$は、現在の状態$s_{t}$と、エージェントの環境に対する行動$a_{t}$によって決まります。\r\n",
        "\r\n",
        "環境の時間変化もまた決定論的ではなく、ある程度ランダムに（≒確率的に）に変化するため、 $f(s_{t},a_{t})$は確率分布となります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO_jJ3py5Elp"
      },
      "source": [
        "-----\r\n",
        "\r\n",
        "$$r_{t + 1} = R(s_{t},a_{t})$$\r\n",
        "\r\n",
        "報酬$R(s_{t},a_{t})$は、（エージェントが取った行動による）変化によって、環境がどの程度 良い or 悪い 状況となったかを示しています。\r\n",
        "\r\n",
        "このような変化を、後で活用可能な経験として捉えるためには、前の状態$s_{t}$、現在の状態 $s_{t + 1}$、エージェントによる前の動作$a_{t}$という要素全てが必要となります。\r\n",
        "\r\n",
        "現在の状態は$s_{t + 1} \\sim f(s_{t},a_{t})$でモデル化できるため、報酬自体は単に $s_{t}$と$s_{t + 1}$から決まる関数の値と捉えることが可能です。\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qjct2y9a8um"
      },
      "source": [
        "## 利得 （Return）\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FimOrNBVb4lM"
      },
      "source": [
        "### 行動軌跡 （Trajectory）\n",
        "\n",
        "$$\\tau = (s_{0},a_{0},s_{1},a_{1},...,s_{T})$$\n",
        "\n",
        "（日本語訳注：原文では省略されていますが、後の記述で登場するため、日本語版では左辺に$\\tau$を追記しています。）\n",
        "\n",
        "<br>\n",
        "\n",
        "**行動軌跡 （Trajectory）**とは状態と行動の系列で、強化学習において発生した全てのプロセスを表します。\n",
        "\n",
        "なお、最後の状態である$s_{T}$が存在するのは、エピソード的な性質があるプロセスにおいてのみです。\n",
        "\n",
        "（日本語訳注：「エピソード的なプロセス」とは環境に開始と終わりがあるようなプロセスです。）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1iv7X9hodh1"
      },
      "source": [
        "\r\n",
        "\r\n",
        "また、報酬は行動軌跡 に含まれていない点には注意してください。\r\n",
        "\r\n",
        "報酬は、「エージェントが行動選択の方法を変更するために返される信号」として捉えることができますが、報酬自体には行動軌跡に関する情報には含まれません。\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lmIKuxsb6qu"
      },
      "source": [
        "### 利得（Return）\n",
        "\n",
        "$$R(\\tau) = \\sum_{t = 0}^{T}r_{t}\\gamma^{t}$$\n",
        "\n",
        "**利得（Return）**は、割引率$\\gamma$が適用された行動軌跡全体にわたる報酬の累積であり、$\\sum_{t = 0}^{T}r_{t}\\gamma^{t}$という式で表現されます。\n",
        "\n",
        "割引率$\\gamma$（時間割引率とも呼ぶ）が無い場合、利得はエージェントが得る報酬の単なる総和となります。\n",
        "\n",
        "割引率$\\gamma$によって、未来の報酬よりも、より直近の報酬の獲得に重きを置くことが可能になります。\n",
        "\n",
        "また同時に、割引率$\\gamma$は利得が発散して無限になることを防いでおり、数学的な面でも役立っています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysaLryRdb8wV"
      },
      "source": [
        "### 状態価値関数（Value function）\n",
        "\n",
        "$$V^{\\pi}(s) = \\mathbf{E}_{\\tau \\sim \\pi}\\lbrack R(\\tau)|s_{0} = s\\rbrack$$\n",
        "\n",
        "強化学習のタスクでは、ある状態$s$における利得$R(\\tau)$を推定したい、というケースがよくあります。\n",
        "\n",
        "このような利得の期待値を求めるための関数を、**状態価値関数（Value function）**と呼びます。\n",
        "\n",
        "<br>\n",
        "\n",
        "この状態価値関数は状態$s$から始まる行動軌跡$\\tau$全体に関係するため、必然的に、方策に依存する形で決まることになります。\n",
        "\n",
        "（日本語訳注：行動軌跡$\\tau$を構成する要素である行動$a$が方策によって決まるためです。）\n",
        "\n",
        "このように方策によって行動軌跡が決まることを示すために、期待値の計算の中では$\\tau \\sim \\pi$を用いています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo_Jwt5sb-K2"
      },
      "source": [
        "### 行動価値関数（Action value function）\n",
        "$$\n",
        "Q^{\\pi}(s, a) = \\mathbf{E}_{\\tau \\sim \\pi}[R(\\tau) | s_0 = s, a_0 = a]\n",
        "$$\n",
        "\n",
        "状態価値関数と似た概念である**行動価値関数（Action value function）**は、エージェントがとある状態$s$からスタートして特定の行動を取った場合の、利得の期待値をモデル化したものです。\n",
        "\n",
        "（一つ前に紹介した）状態価値関数は、ある開始状態$s$において、エージェントが方策$a \\sim \\pi(s)$に従って最初の行動を取る時の利得をモデリングしたものです。\n",
        "\n",
        "これに対し、行動価値関数は、ある開始状態$s$において、エージェントが具体的に行動$a$を採用した場合の利得をモデリングしたものです。\n",
        "\n",
        "（日本語訳注：状態価値関数はまだ行動を<u>取っていない</u>状態の利得を算出するモデルで、行動価値関数は行動を<u>既に取った</u>状態の利得を算出するモデル、と捉えるとイメージがしやすいと思います。）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBfF2lWnpuft"
      },
      "source": [
        "\r\n",
        "\r\n",
        "![pic](https://drive.google.com/uc?id=1Dfijscr2WLJMrypF-ACD_ZwiXLrl8vsw)\r\n",
        "\r\n",
        "\r\n",
        "上図の$s$から$s′$への状態遷移を例にとって考えてみましょう。\r\n",
        "\r\n",
        "エージェントは$a_{1},a_{2},a_{3}$のいずれかの行動を選択することができます。\r\n",
        "\r\n",
        "行動価値関数は、いずれか一つの選択された行動を条件として、利得の期待値をモデリングしています。\r\n",
        "\r\n",
        "一方、状態価値関数は、（行動が方策からサンプリングされている）全ての行動に対する行動価値関数の期待値をモデリングしたものとなっています。\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "（日本語訳注：以下の数式は、状態価値関数と行動価値関数の関係を示したものです）\r\n",
        "\r\n",
        "$$\r\n",
        "V^{\\pi}(s) = \\mathbf{E}_{a \\sim \\pi(s)}[Q^{\\pi}(s, a)]\r\n",
        "$$\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM05n9DrbJrP"
      },
      "source": [
        "最適な利得 \n",
        "------------\n",
        "\n",
        "報酬/利得は、エージェントの学習を促します。\n",
        "\n",
        "言い換えると、エージェントは、得られる全体の利得を最大化するように行動しようとします。\n",
        "\n",
        "強化学習の枠組みでは、ある開始状態と最初のタイムステップでの行動が与えられた場合における最大の利得を知りたい、というモチベーションがあります。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAckka2ScDla"
      },
      "source": [
        "\n",
        "### 最適状態価値関数 （Optimal value function）\n",
        "$$\n",
        "V^*(s) = \\max_{\\pi} \\mathbf{E}_{\\tau \\sim \\pi}[R(\\tau) | s_0 = s]\n",
        "$$\n",
        "\n",
        "**最適状態価値関数（Optimal value function）**とは、最良の方策*を用いた場合における状態価値関数のことです。\n",
        "\n",
        "ここでの「最良」とは、最も高い状態価値を実現する、ということを意味しています。\n",
        "\n",
        "（実際の方策ではなく）このような「最適≒最良の方策」を仮定していることを示すために、$\\pi$の代わりに$*$を用いています。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snRMrCIccEx8"
      },
      "source": [
        "\n",
        "### 最適行動価値関数 （Optimal action value function）\n",
        "$$\n",
        "Q^*(s, a) = \\max_{\\pi} \\mathbf{E}_{\\tau \\sim \\pi}[R(\\tau) | s_0 = s, a_0 = a]\n",
        "$$\n",
        "\n",
        "（最適状態価値関数と）同様に、**最適行動価値関数（Optimal action value function）**は、最適な方策を用いた場合の行動価値関数のことです。\n",
        "\n",
        "なお最初に取られる行動は $a$ として与えられているので、最適な方策が使用されるのは 2 番目以降の行動となります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHDIDFmeBFbL"
      },
      "source": [
        "-----\r\n",
        "\r\n",
        "状態価値関数や行動価値関数の定義とは異なり、上記の２つの最適化された関数は、実際の方策とは関係が無いという点には注意してください。\r\n",
        "\r\n",
        "これらの最適化された関数は、実際に使用されている方策とは無関係に、ある開始状態（と行動）のもとで得られる最大の利得を返すような方策です。\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "（日本語訳注：最適化された関数である$Q^{*}(s,a)$や$V^{*}(s)$は理想の関数であり、ほとんどの場合実際には分からないものです。\r\n",
        "もし仮に真の$Q^{*}(s,a)$や$V^{*}(s)$が分かっている場合には、その結果から直接「理想的な行動」を求めることができるようになります。）\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF42_O3CqQL1"
      },
      "source": [
        "\r\n",
        "Qラーニング・アルゴリズムにおけるQは、最適行動価値関数を指します。\r\n",
        "\r\n",
        "最適なQ関数（最適行動価値関数）のモデルを用いることで、とある状態における最適な行動を簡単に得ることができるようになります。\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "（日本語訳注：Qラーニングについては、本チュートリアルの最後に紹介します）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ313skqbSjQ"
      },
      "source": [
        "## 最適行動（Optimal Action）\n",
        "\n",
        "$$\n",
        "a^*(s) = argmax_a Q^*(s, a)\n",
        "$$\n",
        "\n",
        "状態$s$における最適行動価値関数$Q^{∗}$ がある場合には、最適な行動$a^{*}$は「$Q^{*}$の出力値が最も高い行動」となります。\n",
        "\n",
        "この最適行動$a^{*}$は、単に次の状態における報酬を最大化するだけではな く、将来の全タイムステップを考慮に入れ、行動軌跡全体から決まる、利得を最大化します。\n",
        "\n",
        "上記のような最適行動$a^{*}$という関数は、エージェントが取りうる方策関数の一つでもあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLXHWFngbNzw"
      },
      "source": [
        "## ベルマン方程式 \n",
        "\n",
        "![pic](https://drive.google.com/uc?id=1Dfijscr2WLJMrypF-ACD_ZwiXLrl8vsw)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I2PjbdPcI3c"
      },
      "source": [
        "### 状態価値関数のベルマン方程式\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathbf{E}_{a \\sim \\pi(s), s' \\sim f(s,a)}[r(s, a) + \\gamma V^{\\pi}(s')]\n",
        "$$\n",
        "\n",
        "$\\gamma$は割引率、$\\pi$は方策、$f(s, a)$は環境のモデル、$s'$は次の時間ステップの状態を表します。\n",
        "\n",
        "また、上記式を見ると、現在の状態の利得の期待値である $V^{\\pi}(s)$ を計算するためには、以下の二つの和を取る必要があることが分かります。\n",
        "\n",
        "- 現在の時間ステップにおける報酬$r(s,a)$\n",
        "- 次の時間ステップの状態に対する、割引きされた利得の期待値$\\gamma V^{\\pi}(s\\prime)$\n",
        "\n",
        "現在の時間ステップにおける報酬$r(s,a)$は取られた行動$a$に依存しており、行動行動$a$は、$a \\sim \\pi(s)$という形でモデル化されています。\n",
        "\n",
        "また、次の状態における利得の期待値は単に$V^{\\pi}(s')$と表され、カッコ内の$s'$は次の時間ステップでの状態を表しています。\n",
        "\n",
        "そしてこの$s'$を実際に求めるには、次の状態を得るためのモデル$s'$ $\\sim f(s,a)$を使うことができます。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atoad6YFcKNV"
      },
      "source": [
        "### 行動価値関数のベルマン方程式\n",
        "\n",
        "$$\n",
        "Q^{\\pi}(s, a) = r(s, a) + \\gamma \\mathbf{E}_{a' \\sim \\pi(s'), s' \\sim f(s,a)}[Q^{\\pi}(s', a')]\n",
        "$$\n",
        "\n",
        "$a'$は、方策$\\pi(s')$からサンプリングされた次の時間ステップでの行動です。\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "現在の状態・行動における$Q^{\\pi}(s, a)$の利得の期待値は、「状態価値関数のベルマン方程式」と同様に、以下の二つの和として計算されます。\n",
        "\n",
        "- 現在の時間ステップにおける報酬$r(s, a)$\n",
        "- 次の時間ステップの状態・行動における行動価値関数$Q^{\\pi}(s', a')$\n",
        "\n",
        "<br>\n",
        "\n",
        "現在の行動$a$は予め与えられているので、方策$\\pi$に基づいて選択する必要はありません。\n",
        "\n",
        "しかし、今回は次の時間ステップにおける状態と行動を両方とも使うことになります。\n",
        "\n",
        "そこで、まず次のステップの状態を$s'\\sim f(s,a) $としてモデル化し、次の行動を$a'\\sim\\pi(s')$としてサンプリングする必要があります。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpV_u3HiCH5j"
      },
      "source": [
        "-----\r\n",
        "ベルマン方程式は、現在の状態における価値関数と、次の状態における価値関数の関係を表しています。\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "（日本語訳注：ここにおける「価値関数」は、状態価値関数もしくは行動価値関数、という意味です）\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "ベルマン方程式を使うことで、連続した各状態の価値関数を連鎖させることができます。\r\n",
        "\r\n",
        "ベルマン方程式によって、各状態の利得の期待値を、次の状態の利得の期待値に基づいて算出するようにモデル化することができ、最終的には最後の状態$s_{T}$で連鎖を止めることが可能となっています。\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYx8xl46bWgY"
      },
      "source": [
        "## 最適ベルマン方程式 \n",
        "\n",
        "最適ベルマン方程式とは、最適な行動を採用する場合のベルマン方程式です。\n",
        "\n",
        "言い換えると、最適ベルマン方程式を実現するためには、シンプルに、与えられた状態に対して最適な行動を選択すればよい、ということです。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwcHEXBzcNr2"
      },
      "source": [
        "### 状態価値関数の最適ベルマン方程式\n",
        "$$\n",
        "V^*(s) = \\max_a\\mathbf{E}_{s' \\sim f(s,a)}[r(s, a) + \\gamma V^*(s')]\n",
        "$$\n",
        "最適ベルマン方程式では、方策$\\pi(s)$から行動$a$をサンプリングするのではなく、現在の状態$s$における最適な行動を選択する、という点に注意してください。\n",
        "\n",
        "最適ベルマン方程式を達成するような行動は、すなわち最適な行動でもあります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNaOc4flcOl2"
      },
      "source": [
        "\n",
        "### 行動価値関数の最適ベルマン方程式\n",
        "$$\n",
        "Q^*(s, a) = r(s, a) + \\gamma \\max_{a'}\\mathbf{E}_{s' \\sim f(s,a)}[Q^*(s', a')]\n",
        "$$\n",
        "\n",
        "ここでは、次の状態$s'$における最適な行動$a'$が必要となります。\n",
        "\n",
        "<br>\n",
        "\n",
        "ある方策に基づく行動価値関数 $Q^{\\pi}(s, a)$よりも、最適な行動価値関数 $Q^*(s, a)$を知りたいのと同様に、ベルマン方程式においても、より知りたいことは最適行動価値関数が得られるベルマン方程式です。\n",
        "\n",
        "最適ベルマン方程式は、任意の与えられた状態・行動において、可能な限り最大の利得を表します。\n",
        "\n",
        "<br>\n",
        "\n",
        "上記の式において最も重要なことは、エージェントが現在の状態での利得を最大化する行動（最適状態価値関数から決まる）と、次の時間ステップでの状態（最適行動値関数から決まる）での利得を最大化する行動を選択すると、エージェントは最適行動価値関数$Q^*$を用いて、全体を通じた最大の利得が得られる、という点です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41G6q_5NbaGg"
      },
      "source": [
        "## TD（Temporal Difference）学習\n",
        "\n",
        "TD（Temporal Differenc）学習は、未来の状態の推定値を用いて、現在の状態における推定値を改善していく手法です。\n",
        "\n",
        "<br>\n",
        "\n",
        "この手法の背景にある仮説は、次の状態への遷移について（≒現在の状態から次の状態に移る際の報酬 $r$ について）知っていることが増えると、現在の状態における利得をより良く推定できる、というものです\n",
        "\n",
        "$$\n",
        "V^{\\pi}(s) \\leftarrow V^{\\pi}(s)+\\alpha (r+\\gamma V^{\\pi}(s') -V^{\\pi}(s))\n",
        "$$\n",
        "\n",
        "上記数式の $s$は現在の状態 、$s'$は次の時間ステップでの状態を表しています。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2abP5k2kcRnn"
      },
      "source": [
        "### TD推定値（TD Estimate）\n",
        "\n",
        "$V^{\\pi}(s)$は、**TD推定値（TD Estimate）**と呼ばれます。\n",
        "\n",
        "これは、現在の状態における状態価値関数を推定したものです。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q072-fLecSkb"
      },
      "source": [
        "### TDターゲット （TD Target）\n",
        "\n",
        "$r+ \\gamma V^{\\pi}(s')$を**TDターゲット（TD Target）**と呼びます。\n",
        "\n",
        "TDターゲットはお手本となる関数であり、このTD ターゲットにTD 推定値を近似させたい、というモチベーションがあります。\n",
        "\n",
        "<br>\n",
        "\n",
        "TDターゲットは、現在の報酬$r$と、次の状態の利得の推定値$\\gamma V^{\\pi}(s')$に基づいて算出されます。\n",
        "\n",
        "TD 推定値に比べ、TDターゲットには $r$ が追加されているため、推論結果がより正確な値となります。\n",
        "\n",
        "これを教師あり学習に例えると、$r+\\gamma V^{\\pi}(s')$が「教師ラベル」で、$V^{\\pi}(s)$が「推論結果」となるイメージです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNlNabz2EMYY"
      },
      "source": [
        "-----\r\n",
        "TD学習は、状態価値関数$V^{\\pi}(s)$と行動価値関数$Q^{\\pi}(s, a)$の両方に適用することが可能です。\r\n",
        "\r\n",
        "行動価値関数$Q^{\\pi}(s, a)$に適用した場合、TD学習は次のような数式で表現されます。\r\n",
        "\r\n",
        "$$Q^{\\pi}(s,a) \\leftarrow Q^{\\pi}(s,a) + \\alpha(r + \\gamma Q^{\\pi}(s\\prime,a\\prime) - Q^{\\pi}(s,a))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBny3BgNbcmh"
      },
      "source": [
        "## Ｑラーニング \n",
        "\n",
        "Qラーニングは、TD学習と最適ベルマン方程式を組み合わせた手法です。\n",
        "\n",
        "<br>\n",
        "\n",
        "QラーニングはTD学習の考え方に基づき、報酬$r$と、次の時間ステップでの状態・行動における最適な推定値$\\gamma\\text{max}Q^{*}(s\\prime,a\\prime)$を用いて、現在の状態・行動におけるより良い推定値を算出します。\n",
        "\n",
        "$$Q^{*}(s,a) \\leftarrow Q^{*}(s,a) + \\alpha(r + \\gamma\\underset{a\\prime}{\\text{max}}Q^{*}(s\\prime,a\\prime) - Q^{*}(s,a))$$\n",
        "\n",
        "$r+\\gamma \\underset{a\\prime}{\\text{max}}Q^{*}(s\\prime,a\\prime)$という式は、一つ前のセクションで「TD ターゲット」として定義したものと同じである点に注意してください。\n",
        "\n",
        "この**TDターゲット**の方が、TD推定値よりも、現在の状態・行動に対する推定が優れているため、これに近づくようにTD推定値である$Q^*(s, a)$を更新したい、というのがTD学習におけるコンセプトです。\n",
        "\n",
        "<br>\n",
        "\n",
        "これはつまり、TD推定値である $Q^*(s, a)$と、TDターゲットである$r+\\gamma \\underset{a\\prime}{\\text{max}}Q^{*}(s\\prime,a\\prime)$の差を最小にしたい、ということを意味します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jarlFCKeEXaV"
      },
      "source": [
        "$$L(\\theta) = (r + \\gamma\\underset{a\\prime}{\\text{max}}Q_{\\theta}^{*}(s\\prime,a\\prime) - Q_{\\theta}^{*}(s,a))^{2}$$\r\n",
        "\r\n",
        "最適行動価値関数 $Q^*(s', a')$ にはニューラル・ネットワークを使うことが多いため、ここでは$\\theta$はニューラルネットワークのパラメータを表します。\r\n",
        "\r\n",
        "ここでの目標は、$\\theta$に関する損失$L(\\theta) $を最小化するために、$Q_{\\theta}^*(s, a)$を$r+\\gamma \\underset{a\\prime}{\\text{max}}Q^{*}(s\\prime,a\\prime)$に近づけることとなります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a3r4IY4vLq9"
      },
      "source": [
        "以上。"
      ]
    }
  ]
}