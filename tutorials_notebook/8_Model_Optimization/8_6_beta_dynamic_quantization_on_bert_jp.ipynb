{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "8_6_beta_dynamic_quantization_on_bert_jp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEpwCFuGnm90"
      },
      "source": [
        "# 「BERTの動的量子化（ベータ版）」\n",
        "\n",
        "【原題】(beta) Dynamic Quantization on BERT\n",
        "\n",
        "【原著】[Jianyu Huang](https://github.com/jianyuh)\n",
        "\n",
        "【査読】[Raghuraman Krishnamoorthi](https://github.com/raghuramank100)\n",
        "\n",
        "【編著】[Jessica Lin](https://github.com/jlin27)\n",
        "\n",
        "【元URL】https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html\n",
        "\n",
        "【翻訳】電通国際情報サービスISID HCM事業部　櫻井 亮佑\n",
        "\n",
        "【日付】2020年2月6日\n",
        "\n",
        "【チュトーリアル概要】\n",
        "\n",
        "本チュートリアルでは、HuggingFace TransformersのBERTモデルに動的量子化を適用します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpHjsDK6nm9-"
      },
      "source": [
        "**ヒント**\n",
        "\n",
        "本チュートリアルを最大限活用するには、こちらの[バージョンのColab](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/dynamic_quantization_bert_tutorial.ipynb)を使用することを推奨します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFSixVoAnm-A"
      },
      "source": [
        "## 導入\n",
        "\n",
        "本チュートリアルでは、[HuggingFace Transformers](https://github.com/huggingface/transformers)のBERTモデルに動的量子化を適用します。\n",
        "\n",
        "BERTのような有名かつ最先端のモデルを、動的量子化済みのモデルへと変換する方法について、ひとつずつ解説します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl5ati8iTpCA"
      },
      "source": [
        "\r\n",
        "- BERT、あるいはトランスフォーマーによる双方向型埋め込み表現（Bidirectional Embedding Representations from Transformers）は、質問回答や文書分類などの多くの自然言語処理（NLP）タスクにおいて最先端の精度を達成している、新しい訓練済みの言語表現の方法の一つです。\r\n",
        "  元の論文は[こちら](https://arxiv.org/pdf/1810.04805.pdf)で確認できます。\r\n",
        "\r\n",
        "- PyTorchでサポートされている動的量子化は、重みや活性化について動的量子化を行い、float型のモデルを静的なint8型やfloat16型の量子化モデルへと変換します。\r\n",
        "  \r\n",
        "  活性化は重みがint8型に量子化される際に、（バッチ毎に）動的にint8型へと量子化されます。\r\n",
        "  \r\n",
        "  PyTorchでは、特定のモジュールの重みのみ量子化したものに置換し、量子化モデルを出力する[torch.quantization.quantize_dynamic API](https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic)が存在します。\r\n",
        "\r\n",
        "- 一般的な言語理解評価のベンチマーク[(GLUE)](https://gluebenchmark.com/)の[Microsoft Research Paraphrase Corpus (MRPC)タスク](https://www.microsoft.com/en-us/download/details.aspx?id=52398)について、精度と推論のパフォーマンスの結果を求めます。\r\n",
        "  MRPC (Dolan、Brockett, 2005)は、オンラインのニュースソースから自動的に抽出した文章のペアのコーパスであり、ペアの文章が意味的に等価かどうか人間がアノテーションを付けています。\r\n",
        "  MRPCのクラスは不均衡（68%の陽性、32%の陰性）であるため、一般的な慣習に従い、[F1スコア](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)を指標にします。\r\n",
        "  \r\n",
        "  なお、以下に示すように、MRPCは言語ペアの分類において一般的なNLPのタスクです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAnSGYbunm-A"
      },
      "source": [
        "<img src='https://pytorch.org/tutorials/_images/bert.png'　 width=\"400\" ></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRMLcprcnm-B"
      },
      "source": [
        "## 1. 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBG7OpDTnm-B"
      },
      "source": [
        "### 1.1 PyTorchとHuggingFaceのTransformersのインストール\n",
        "\n",
        "本チュートリアルを始めるにあたって、[PyTorch](https://github.com/pytorch/pytorch/#installation)と[HuggingFaceのGithubのリポジトリ](https://github.com/huggingface/transformers#installation)にあるインストール方法に倣いましょう。\n",
        "さらに、組み込み済みのF1スコアを算出する関数を利用するため、[scikit-learn](https://github.com/scikit-learn/scikit-learn)もインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ce3JL8hnm-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc3cbfe-9670-4f35-9061-ddb93b8821d6"
      },
      "source": [
        "!pip install sklearn\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 5.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=f9c7195f69375d43d5563efe3738c44005f7936733ed5cec7a255b7145f9a0c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jflqjv1Gnm-D"
      },
      "source": [
        "なお、PyTorchのベータ版である機能を使用することになるため、最新バージョンのtorchとtorchvisionをインストールすることを推奨します。\n",
        "最新のローカルでのインストール方法は[こちら](https://pytorch.org/get-started/locally/)で確認可能です。<br>\n",
        "例えば、Mac上にインストールするには下記のコマンドを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z45VOuB8nm-D"
      },
      "source": [
        "# !yes y | pip uninstall torch tochvision\n",
        "# !yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkTSEwBSnm-E"
      },
      "source": [
        "### 1.2 必須モジュールのインポート\n",
        "\n",
        "本ステップでは、チュートリアルを進めるにあたって必須であるPythonのモジュールをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggh3Bxhcnm-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8d8670-452b-4b17-a37e-076fe050d56b"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from argparse import Namespace\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from tqdm import tqdm\n",
        "from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\n",
        "from transformers import glue_compute_metrics as compute_metrics\n",
        "from transformers import glue_output_modes as output_modes\n",
        "from transformers import glue_processors as processors\n",
        "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
        "\n",
        "# ログの準備\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.WARN)\n",
        "\n",
        "logging.getLogger(\"transformers.modeling_utils\").setLevel(\n",
        "   logging.WARN)  # ログの削減\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op_ytnqJnm-F"
      },
      "source": [
        "FP32とINT8との間で、単一スレッドでのパフォーマンスを比較するために、スレッド数を設定します。\n",
        "\n",
        "なお本チュートリアルの最後では、適切な並列バックエンドを持つPyTorchを構築することで、別途スレッド数を設定できるようになります。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ip4sAKnm-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795adaa5-ee7e-4b91-a18a-3127dc5c1fb8"
      },
      "source": [
        "torch.set_num_threads(1)\n",
        "print(torch.__config__.parallel_info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ATen/Parallel:\n",
            "\tat::get_num_threads() : 1\n",
            "\tat::get_num_interop_threads() : 1\n",
            "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "\tomp_get_max_threads() : 1\n",
            "Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "\tmkl_get_max_threads() : 1\n",
            "Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
            "std::thread::hardware_concurrency() : 2\n",
            "Environment variables:\n",
            "\tOMP_NUM_THREADS : [not set]\n",
            "\tMKL_NUM_THREADS : [not set]\n",
            "ATen parallel backend: OpenMP\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60Qvtkw9nm-F"
      },
      "source": [
        "### 1.3 ヘルパー関数について学ぶ\n",
        "\n",
        "transformersのライブラリには組み込み済みのヘルパー関数が存在します。<br>\n",
        "本チュートリアルでは、主に2つのヘルパー関数を使用します。\n",
        "\n",
        "一つは文章のサンプルを特徴ベクトルに変換する関数であり、もう一つは予測結果のF1スコアを測定する関数です。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUdkf8H0U2vb"
      },
      "source": [
        "[glue_convert_examples_to_features](https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py)関数は、文章を入力特徴量に変換します。\r\n",
        "\r\n",
        "- 入力系列をトークン化します。\r\n",
        "- 始めの部分に[CLS]を挿入します。\r\n",
        "- 初めの文章と2番目の文章との間、及び最後の部分に[SEP]を挿入します。\r\n",
        "- トークンが最初の文章に属するか、2番目の文章に属するかを示すトークン型IDを生成します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GE3ZS4Nnm-G"
      },
      "source": [
        "[glue_compute_metrics](https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py)関数は、適合率と再現率の加重平均と解釈できる、[F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)の指標を備えています。\n",
        "\n",
        "F1スコアは最良の値で1を取り、最悪の場合は0を取ります。\n",
        "\n",
        "なお、F1スコアに対する適合率と再現率の寄与は相対的に等しいものになっています。\n",
        "\n",
        "- F1スコアの式は以下のとおりです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBLyU1kfnm-G"
      },
      "source": [
        "$$\n",
        "F1 = 2 * (\\text{precision} * \\text{recall}) / (\\text{precision} + \\text{recall})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnL-Jq_9nm-G"
      },
      "source": [
        "### 1.4 データセットのダウンロード\n",
        "\n",
        "MRPCタスクを実行する前に、[こちらのスクリプト](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)を実行して[GLUEデータ](https://gluebenchmark.com/tasks)をダウンロードし、`glue_data`ディレクトリに解凍します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebRX2idEVT6k"
      },
      "source": [
        "!python download_glue_data.py --data_dir='glue_data' --tasks='MRPC'\r\n",
        "\r\n",
        "# 日本語版注釈：ここがうまく動作しない・・・"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06H92X06nm-H"
      },
      "source": [
        "## 2. BERTモデルのファインチューン\n",
        "\n",
        "言語表現を事前訓練し、様々なタスクに対して、最小限のタスク依存のパラメータを使用してディープな双方向表現をファインチューニングし、最先端の結果を達成する\n",
        "\n",
        "という流れが、BERTのアイデアです。\n",
        "\n",
        "本チュートリアルでは、MRPCタスクに存在する、意味的に等価な文章のペアを分類する事前訓練済みのBERTモデルをファインチューニングすることに専念します。\n",
        "\n",
        "事前訓練済みのBERTモデル（HuggingFaceのtransformers内の`bert-base-uncased`モデル）を、MRPCタスクに対してファインチューニングするには、[こちらのサンプル](https://github.com/huggingface/transformers/tree/master/examples#mrpc)のコマンドにならいます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeUTnbj6nm-H"
      },
      "source": [
        "!export GLUE_DIR=./glue_data\n",
        "!export TASK_NAME=MRPC\n",
        "!export OUT_DIR=./$TASK_NAME/\n",
        "!python ./run_glue.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-uncased \\\n",
        "    --task_name $TASK_NAME \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_lower_case \\\n",
        "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=8   \\\n",
        "    --per_gpu_train_batch_size=8   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --save_steps 100000 \\\n",
        "    --output_dir $OUT_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwLOW8_dnm-I"
      },
      "source": [
        "MRPCタスクのためにファインチューン済みのBERTモデルを[こちら](https://download.pytorch.org/tutorial/MRPC.zip)で提供しています。\n",
        "\n",
        "時間を節約するために、ローカルの`$OUT_DIR`フォルダにモデルのファイル（~400 MB）をダウンロードすることが可能です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6oPDKZrnm-I"
      },
      "source": [
        "### 2.1 グローバルな設定\n",
        "\n",
        "動的量子化の前後でファインチューンされたBERTモデルの評価を行うために、グローバルな設定を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HePmofknm-I"
      },
      "source": [
        "configs = Namespace()\n",
        "\n",
        "# ファインチューン済みモデルを出力するディレクトリ、$OUT_DIR\n",
        "configs.output_dir = \"./MRPC/\"\n",
        "\n",
        "# GLUEベンチマークのMRPCタスクのデータを格納するディレクトリ、$GLUE_DIR/$TASK_NAME\n",
        "configs.data_dir = \"./glue_data/MRPC\"\n",
        "\n",
        "# 事前訓練済みモデルのモデル名、またはパス\n",
        "configs.model_name_or_path = \"bert-base-uncased\"\n",
        "# 入力系列の最大長\n",
        "configs.max_seq_length = 128\n",
        "\n",
        "# GLUEタスクの準備\n",
        "configs.task_name = \"MRPC\".lower()\n",
        "configs.processor = processors[configs.task_name]()\n",
        "configs.output_mode = output_modes[configs.task_name]\n",
        "configs.label_list = configs.processor.get_labels()\n",
        "configs.model_type = \"bert\".lower()\n",
        "configs.do_lower_case = True\n",
        "\n",
        "# デバイス、バッチサイズ、トポロジ（GPU数やマシンのランク）、及びキャッシュフラグを設定\n",
        "configs.device = \"cpu\"\n",
        "configs.per_gpu_eval_batch_size = 8\n",
        "configs.n_gpu = 0\n",
        "configs.local_rank = -1\n",
        "configs.overwrite_cache = False\n",
        "\n",
        "\n",
        "# 再現性のための乱数シードの設定\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "set_seed(42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObduaKxRnm-J"
      },
      "source": [
        "### 2.2 ファインチューン済みBERTモデルの読み込み\n",
        "\n",
        "`configs.output_dir`からトークナイザ―とファインチューニング済みのBERT系列分類器モデル（FP32）を読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsXxaRPunm-J"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    configs.output_dir, do_lower_case=configs.do_lower_case)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(configs.output_dir)\n",
        "model.to(configs.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brvKTDtDnm-K"
      },
      "source": [
        "### 2.3 トークン化と評価を行う関数の定義\n",
        "\n",
        "[Huggingface](https://github.com/huggingface/transformers/blob/master/examples/run_glue.py)からトークン化と評価を行う関数を再利用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPFFgsHvnm-L"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    # MNLIの2つの評価(一致、不一致)を処理するためのループ \n",
        "    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n",
        "    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n",
        "\n",
        "    results = {}\n",
        "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
        "        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
        "\n",
        "        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "            os.makedirs(eval_output_dir)\n",
        "\n",
        "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "        # DistributedSamplerはランダムにサンプリングする点に留意してください。\n",
        "        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        # マルチGPUの確認\n",
        "        if args.n_gpu > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "\n",
        "        # 評価\n",
        "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        preds = None\n",
        "        out_label_ids = None\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            model.eval()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = {'input_ids':      batch[0],\n",
        "                          'attention_mask': batch[1],\n",
        "                          'labels':         batch[3]}\n",
        "                if args.model_type != 'distilbert':\n",
        "                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM、DistilBERT、そしてRoBERTaはsegment_idsを使用しません。\n",
        "                outputs = model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "                eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "                out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        if args.output_mode == \"classification\":\n",
        "            preds = np.argmax(preds, axis=1)\n",
        "        elif args.output_mode == \"regression\":\n",
        "            preds = np.squeeze(preds)\n",
        "        result = compute_metrics(eval_task, preds, out_label_ids)\n",
        "        results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "            for key in sorted(result.keys()):\n",
        "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n",
        "    if args.local_rank not in [-1, 0] and not evaluate:\n",
        "        torch.distributed.barrier()  # 分散訓練内の最初のプロセスのみがデータセットを処理し、その他のプロセスはキャッシュを使用するように担保します。\n",
        "\n",
        "    processor = processors[task]()\n",
        "    output_mode = output_modes[task]\n",
        "    # キャッシュ、またはデータセットのファイルからデータの特徴量を読み込み\n",
        "    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n",
        "        'dev' if evaluate else 'train',\n",
        "        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n",
        "        str(args.max_seq_length),\n",
        "        str(task)))\n",
        "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        label_list = processor.get_labels()\n",
        "        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']:\n",
        "            # ハック：RoBERTaの訓練済みモデルではラベルのインデックスが反転しています。\n",
        "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
        "        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n",
        "        features = convert_examples_to_features(examples,\n",
        "                                                tokenizer,\n",
        "                                                label_list=label_list,\n",
        "                                                max_length=args.max_seq_length,\n",
        "                                                output_mode=output_mode,\n",
        "                                                pad_on_left=bool(args.model_type in ['xlnet']),                 # xlnetでは左部にパディングを挿入します。\n",
        "                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "                                                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0,\n",
        "        )\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            torch.save(features, cached_features_file)\n",
        "\n",
        "    if args.local_rank == 0 and not evaluate:\n",
        "        torch.distributed.barrier()  # 分散訓練内の最初のプロセスのみがデータセットを処理し、その他のプロセスはキャッシュを使用するように担保します。\n",
        "\n",
        "    # テンソルに変換し、データセットを構築します。\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    if output_mode == \"classification\":\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    elif output_mode == \"regression\":\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "    return dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx52WLudnm-Q"
      },
      "source": [
        "## 3. 動的量子化の適用\n",
        "\n",
        "`torch.quantization.quantize_dynamic`をモデルに対して呼び出し、HuggingFaceのBERTモデルに動的量子化を適用します。\n",
        "\n",
        "具体的には、以下の処理が行われるように引数を指定します。\n",
        "\n",
        "- モデル内のtorch.nn.Linearモジュールが量子化される\n",
        "- 重みが量子化されたint8型の値に変換される"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb8cZrWznm-T"
      },
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "print(quantized_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inBJMCgRnm-T"
      },
      "source": [
        "### 3.1 モデルサイズの確認\n",
        "\n",
        "まずはモデルのサイズを確認してみましょう。\n",
        "モデルサイズが大幅に削減されていることが観測できます。(FP32 総サイズ: 438 MB; INT8 総サイズ: 181 MB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sqnI07vnm-U"
      },
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "print_size_of_model(model)\n",
        "print_size_of_model(quantized_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr4kaRSWnm-U"
      },
      "source": [
        "本チュートリアルで使用されているBERTモデル (`bert-base-uncased`) は、V=30522のボキャブラリーサイズです。\n",
        "\n",
        "つまり、埋め込みサイズが768であれば、埋め込みテーブルの総サイズはおよそ 4 (Bytes/FP32) * 30522 * 768 = 90 MB になります。\n",
        "\n",
        "したがって、量子化の効果で非埋め込みテーブルの部分のモデルサイズは、350 MB (FP32モデル)から90 MB (INT8モデル)に削減されています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZz4Sadhnm-U"
      },
      "source": [
        "### 3.2 推論精度と時間の評価\n",
        "\n",
        "次に、元のFP32型モデルと動的量子化後のINT8型モデルの間で、推論時間と精度の評価を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SEhxVyMnm-U"
      },
      "source": [
        "def time_model_evaluation(model, configs, tokenizer):\n",
        "    eval_start_time = time.time()\n",
        "    result = evaluate(configs, model, tokenizer, prefix=\"\")\n",
        "    eval_end_time = time.time()\n",
        "    eval_duration_time = eval_end_time - eval_start_time\n",
        "    print(result)\n",
        "    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n",
        "\n",
        "# 元のFP32型のBERTモデルを評価\n",
        "time_model_evaluation(model, configs, tokenizer)\n",
        "\n",
        "# 動的量子化後のINT8型のBERTモデルを評価\n",
        "time_model_evaluation(quantized_model, configs, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--M6BR2_nm-W"
      },
      "source": [
        "上記のコードを量子化せずにMacBook Pro上で実行した場合、（MRPCデータセット内の全408個のサンプルに対しての）推論には約160秒必要ですが、量子化を行った場合は約90秒しかかかりません。\n",
        "\n",
        "MacBook Proで量子化済みBERTモデルを用いた推論を実行した結果を以下にまとめます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPKpcvpgnm-W"
      },
      "source": [
        "<code>\n",
        "\n",
        "    | Prec | F1 score | Model Size | 1 thread | 4 threads |\n",
        "    | FP32 |  0.9019  |   438 MB   | 160 sec  | 85 sec    |\n",
        "    | INT8 |  0.902   |   181 MB   |  90 sec  | 46 sec    |\n",
        "\n",
        "</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx37mke_nm-W"
      },
      "source": [
        "MRPCタスクについてファインチューニングされたBERTモデルに対して訓練後に動的量子化を行った場合、0.6%のF1スコアが得られました。\n",
        "\n",
        "なお比較として、[最近の論文](https://arxiv.org/pdf/1910.06188.pdf)(表1)では、訓練後に動的量子化を適用することで0.8788、量子化を考慮した訓練を行うことで0.8956のスコアを達成しています。\n",
        "\n",
        "主な違いは、前記の論文は対称的な量子化のみサポートしている一方で、PyTorch では非対称的な量子化をサポートしている点です。\n",
        "\n",
        "なお、本チュートリアルでは単一スレッドでの比較を行うためにスレッド数を1に設定している点に留意してください。\n",
        "\n",
        "これらの量子化INT8型の演算子の処理の並列化もサポートしています。\n",
        "ユーザーは`torch.set_num_threads(N)`（Nは処理中に並列化するスレッド数）により、マルチスレッドを設定することが可能です。\n",
        "\n",
        "ただし、処理の並列化サポートを有効化するために必要な補足事項として、PyTorchをOpenMP、Native、またはTBBといった適切な[バックエンド](https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#build-options)でビルドする必要があります。\n",
        "\n",
        "`torch.__config__.parallel_info()`を使用し、並列化の設定を確認することが可能です。\n",
        "\n",
        "ちなみに、同一のMacBook Proで並列化にNativeバックエンドを使用したPyTorchでは、MRPCのデータセットの評価の処理を約46秒で行えました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afkX2yO2nm-X"
      },
      "source": [
        "### 3.3 量子化モデルのシリアル化\n",
        "\n",
        "モデルのトレース後に`torch.jit.save`を使用することで、量子化モデルをシリアル化し、保存することが可能です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiEs41Zbnm-X"
      },
      "source": [
        "input_ids = ids_tensor([8, 128], 2)\n",
        "token_type_ids = ids_tensor([8, 128], 2)\n",
        "attention_mask = ids_tensor([8, 128], vocab_size=2)\n",
        "dummy_input = (input_ids, attention_mask, token_type_ids)\n",
        "traced_model = torch.jit.trace(quantized_model, dummy_input)\n",
        "torch.jit.save(traced_model, \"bert_traced_eager_quant.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwFJOlwqnm-X"
      },
      "source": [
        "量子化モデルを読み込むには、`torch.jit.load`が使用可能です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0YImqU1nm-X"
      },
      "source": [
        "loaded_quantized_model = torch.jit.load(\"bert_traced_eager_quant.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyVW6Gv6nm-Y"
      },
      "source": [
        "## 結論\n",
        "\n",
        "本チュートリアルでは、BERTのような有名で最先端のNLPモデルを動的に量子化されたモデルへと変換する方法を解説しました。\n",
        "\n",
        "動的量子化は、精度への影響を限定的なものとしつつ、モデルのサイズを削減します。\n",
        "\n",
        "本チュートリアルに目を通していただき、ありがとうございます。\n",
        "\n",
        "どんなフィードバックでも歓迎するので、もし何かございましたら、[こちら](https://github.com/pytorch/pytorch/issues)にissueを作成してください。\n",
        "\n",
        "（日本語訳注：日本語版チュートリアルに関連する内容であれば、是非[こちら](https://github.com/YutaroOgawa/pytorch_tutorials_jp)にissueの登録をお願いいたします。）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X72N2G1Ynm-Y"
      },
      "source": [
        "## 引用\n",
        "\n",
        "[1] J.Devlin、 M. Chang、 K. Lee、 K. Toutanova, [BERT: 言語理解のための双方向ディープトランスフォーマーの事前訓練(2018)](https://arxiv.org/pdf/1810.04805.pdf).\n",
        "\n",
        "[2] [HuggingFace Transformers](https://github.com/huggingface/transformers).\n",
        "\n",
        "[3] O. Zafrir、 G. Boudoukh、 P. Izsak、M. Wasserblat (2019). [Q8BERT: 量子化8bit BERT](https://arxiv.org/pdf/1910.06188.pdf)."
      ]
    }
  ]
}