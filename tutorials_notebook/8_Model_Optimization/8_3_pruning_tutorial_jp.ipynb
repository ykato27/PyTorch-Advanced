{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "8_3_pruning_tutorial_jp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4M0nTo0kxE9"
      },
      "source": [
        "# 「枝刈り（Pruning）のチュートリアル」\n",
        "\n",
        "【原題】Pruning Tutorial\n",
        "\n",
        "【原著】[Michela Paganini](https://github.com/mickypaganini)\n",
        "\n",
        "【元URL】https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n",
        "\n",
        "【翻訳】電通国際情報サービスISID HCM事業部　櫻井 亮佑\n",
        "\n",
        "【日付】2020年1月26日\n",
        "\n",
        "【チュトーリアル概要】\n",
        "\n",
        "最先端のディープラーニング技術は、非常に多くのパラメータを持つモデルとなっています。\n",
        "\n",
        "このような技術を用いたモデルには、デプロイが困難であるという課題があります。\n",
        "\n",
        "一方で、生物学的な脳内のニューラルネットワークは、効率的で疎な接続を利用することで知られています。\n",
        "\n",
        "パラメータ数を削減することによってモデルを圧縮する最適なテクニックを把握しておくことは、精度を損なわずにメモリ、バッテリー、そしてハードウェアの消耗を減らし、デバイス上に軽量なモデルをデプロイするために重要です。\n",
        "\n",
        "また、プライベートなデバイス（エッジ系）において演算を行う際にプライバシーを確保する上でも重要となります。\n",
        "\n",
        "枝刈りは、ニューラルアーキテクチャの探索テクニックとして、多くのパラメータで構成されているネットワークと、パラメータが少ないネットワークでの学習ダイナミクスの違いを調査するケースや、疎な当たりのサブネットワークと初期化(\"[当たりくじ](https://arxiv.org/abs/1803.03635)\")の役割を研究するケースなど、研究の最前線で使用されています。\n",
        "\n",
        "本チュートリアルでは、`torch.nn.utils.prune` を用いてニューラルネットワークを疎にする方法、及びその方法を任意の枝刈りのテクニックの実装に拡張する方法を学びます。\n",
        "\n",
        "## 必須要件\n",
        "`\"torch>=1.4.0a0+8e8a5e0\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw7ReQY2kxFO"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NwPt5J8kxFR"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyDxJBlokxFS"
      },
      "source": [
        "## モデルの作成\n",
        "\n",
        "本チュートリアルでは、LeCun 1998らの[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)アーキテクチャを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkN9b0WhkxFT"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1つの画像チャネル、6つの出力チャネル、3x3の四角形の畳み込みカーネル\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5の画像次元\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = LeNet().to(device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBWFgv-GkxFU"
      },
      "source": [
        "## モジュールの確認\n",
        "\n",
        "LeNetモデルの（枝刈りされていない）`conv1`層を確認してみましょう。\n",
        "`conv1`層は、`weight` と `bias` の2つのパラメータを含んでおり、この時点でバッファは含んでいません。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZE7RNVukxFU",
        "outputId": "e3cf0917-8c6c-4822-f8ab-01141bdaea8c"
      },
      "source": [
        "module = model.conv1\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight', Parameter containing:\n",
            "tensor([[[[ 0.3273,  0.2712, -0.0953],\n",
            "          [ 0.0764, -0.2651, -0.3249],\n",
            "          [ 0.2157, -0.2530,  0.0524]]],\n",
            "\n",
            "\n",
            "        [[[-0.0895,  0.0482,  0.2638],\n",
            "          [-0.2001, -0.1045,  0.2797],\n",
            "          [-0.3160,  0.3137, -0.1420]]],\n",
            "\n",
            "\n",
            "        [[[-0.0745,  0.2557, -0.0125],\n",
            "          [ 0.0688,  0.2205,  0.2299],\n",
            "          [-0.0944,  0.1338, -0.0233]]],\n",
            "\n",
            "\n",
            "        [[[-0.0590, -0.2994, -0.1391],\n",
            "          [-0.0028,  0.2432, -0.2896],\n",
            "          [-0.0729, -0.2522, -0.2231]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1020, -0.0075, -0.1868],\n",
            "          [ 0.2437,  0.0663, -0.0489],\n",
            "          [ 0.2144, -0.1072, -0.3217]]],\n",
            "\n",
            "\n",
            "        [[[-0.0846,  0.1886,  0.0569],\n",
            "          [-0.2620, -0.2686, -0.0275],\n",
            "          [-0.0430,  0.2285, -0.3022]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\n",
            "tensor([ 0.1315,  0.1902,  0.1169, -0.0872, -0.0136,  0.2106], device='cuda:0',\n",
            "       requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLOS59fPkxFX",
        "outputId": "4de38ab5-e50f-4b84-d621-88bd9f7bd60d"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJIv2h1wkxFX"
      },
      "source": [
        "## モジュールの枝刈り\n",
        "\n",
        "モジュール（今回の例では、LeNetアーキテクチャの`conv1`層）を枝刈りするには、初めに`torch.nn.utils.prune`（または、`BasePruningMethod`をサブクラス化することで独自に実装したもの）で利用できる選択肢から枝刈りのテクニックを指定します。\n",
        "\n",
        "そして、モジュールと当該モジュール内で枝刈りするパラメータを設定します。\n",
        "\n",
        "最後に、選択した枝刈りのテクニックに必要なキーワード引数を用いて、枝刈りを行う際のパラメータを指定します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgxz_mNa_fdK"
      },
      "source": [
        "今回の例では、`conv1`層の`weight`という名前のパラメータ内の接続をランダムに30％枝刈りします。\r\n",
        "\r\n",
        "モジュールは、枝刈りする関数に最初の引数として渡されます。\r\n",
        "\r\n",
        "その他の引数として、`name` は文字列の識別子を用いてモジュール内のパラメータを識別し、`amount`は、（0. から 1.の間のfloatの場合は）枝刈りする接続のパーセンテージ、または（非負のintegerの場合は）枝刈りする接続の絶対数を示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXA_vB3ekxFY",
        "outputId": "19081a41-9631-407d-d317-0aab9c0654d6"
      },
      "source": [
        "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb42DEn0kxFZ"
      },
      "source": [
        "パラメータから`weight`を取り除き、`weight_orig`という新たなパラメータに置き換えることで、枝刈りが行われます（例：初期のパラメータの`name`に`\"_orig\"`を付与されます）。\n",
        "\n",
        "`weight_orig`は、枝刈りが行われていないバージョンのテンソルを保持しています。\n",
        "\n",
        "一方で、`bias`は枝刈りされず、そのままの状態を維持します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNhS8XZJkxFa",
        "outputId": "82f8a4fb-d0c7-4655-a693-60c56d3fc5e6"
      },
      "source": [
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bias', Parameter containing:\n",
            "tensor([ 0.1315,  0.1902,  0.1169, -0.0872, -0.0136,  0.2106], device='cuda:0',\n",
            "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
            "tensor([[[[ 0.3273,  0.2712, -0.0953],\n",
            "          [ 0.0764, -0.2651, -0.3249],\n",
            "          [ 0.2157, -0.2530,  0.0524]]],\n",
            "\n",
            "\n",
            "        [[[-0.0895,  0.0482,  0.2638],\n",
            "          [-0.2001, -0.1045,  0.2797],\n",
            "          [-0.3160,  0.3137, -0.1420]]],\n",
            "\n",
            "\n",
            "        [[[-0.0745,  0.2557, -0.0125],\n",
            "          [ 0.0688,  0.2205,  0.2299],\n",
            "          [-0.0944,  0.1338, -0.0233]]],\n",
            "\n",
            "\n",
            "        [[[-0.0590, -0.2994, -0.1391],\n",
            "          [-0.0028,  0.2432, -0.2896],\n",
            "          [-0.0729, -0.2522, -0.2231]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1020, -0.0075, -0.1868],\n",
            "          [ 0.2437,  0.0663, -0.0489],\n",
            "          [ 0.2144, -0.1072, -0.3217]]],\n",
            "\n",
            "\n",
            "        [[[-0.0846,  0.1886,  0.0569],\n",
            "          [-0.2620, -0.2686, -0.0275],\n",
            "          [-0.0430,  0.2285, -0.3022]]]], device='cuda:0', requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBS17iTkxFb"
      },
      "source": [
        "上で選択された枝刈りのテクニックによって生成された枝刈りのマスクは、`weight_mask`という名前のモジュールバッファとして保存されます（例：初期のパラメータの`name`に`\"_mask\"`が付与されます）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orR8FptokxFb",
        "outputId": "d987898f-d7c4-4b22-af47-53f92195d714"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight_mask', tensor([[[[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 0.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 0.],\n",
            "          [1., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 1.],\n",
            "          [1., 0., 1.],\n",
            "          [1., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 0.],\n",
            "          [1., 1., 0.],\n",
            "          [1., 1., 0.]]]], device='cuda:0'))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDYnvEzFkxFc"
      },
      "source": [
        "何も手を加えずにフォワードパスを機能させるには、モジュールに`weight`属性が存在している必要があります。\n",
        "\n",
        "`torch.nn.utils.prune`で実装されている枝刈りのテクニックは、（マスクを元のパラメータと突き合わせることで）枝刈りされたバージョンの重みを処理し、それらを`weight`属性に格納します。\n",
        "\n",
        "なお、上記のような枝刈りのテクニックを適用した後は、重みが`module`のパラメータではなく、ただの属性変数になっている点に注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLwBxHTPkxFd",
        "outputId": "10bf5487-b0d7-4335-fba7-c1c8e4bd6e7f"
      },
      "source": [
        "print(module.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0764, -0.2651, -0.3249],\n",
            "          [ 0.2157, -0.0000,  0.0524]]],\n",
            "\n",
            "\n",
            "        [[[-0.0895,  0.0482,  0.0000],\n",
            "          [-0.0000, -0.1045,  0.0000],\n",
            "          [-0.3160,  0.3137, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0745,  0.0000, -0.0000],\n",
            "          [ 0.0688,  0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.1391],\n",
            "          [-0.0028,  0.0000, -0.2896],\n",
            "          [-0.0729, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1020, -0.0000, -0.1868],\n",
            "          [ 0.2437,  0.0663, -0.0489],\n",
            "          [ 0.0000, -0.0000, -0.3217]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.1886,  0.0000],\n",
            "          [-0.2620, -0.2686, -0.0000],\n",
            "          [-0.0430,  0.2285, -0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnSAZ9O1kxFd"
      },
      "source": [
        "最後に、各フォワードパスに先立って、PyTorchの`forward_pre_hooks`を使用することで枝刈りを適用します。\n",
        "\n",
        "具体的には、上で行ったように、`module`が枝刈りされる際、対応するパラメータが枝刈りされる度に `forward_pre_hook` が作成されます。\n",
        "\n",
        "今回のケースでは、元が`weight`という名前のパラメータのみを枝刈りしたため、一つのフックが存在することになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7PbDA8EkxFe",
        "outputId": "4128edfb-5818-42a1-d004-a8f62bd60083"
      },
      "source": [
        "print(module._forward_pre_hooks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([(1, <torch.nn.utils.prune.PruningContainer object at 0x7f937097dba8>)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tqqE7rwkxFf"
      },
      "source": [
        "全パラメータに枝刈りを行うために`bias`も枝刈りしましょう。\n",
        "\n",
        "`module`のパラメータ、バッファ、フック、そして属性がどのように変わるか確認することができます。\n",
        "\n",
        "別の枝刈りのテクニックを試すこととし、ここでは`l1_unstructured`という枝刈り関数で実装されている手法を使い、L1ノルムを基準にしてバイアス内の3つの最小の要素を枝刈りします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cRYbR94kxFf",
        "outputId": "f0b1434c-1baa-4848-c036-8296be2fef4f"
      },
      "source": [
        "prune.l1_unstructured(module, name=\"bias\", amount=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y4P2I2BkxFf"
      },
      "source": [
        "これで名前付きパラメータに、`weight_orig`と `bias_orig`の両方が含まれているはずです。\n",
        "\n",
        "またバッファは、`weight_mask` と `bias_mask` を含んでいます。\n",
        "\n",
        "そして、枝刈りされたバージョンの2つのテンソルはモジュールの属性として存在し、この時点でモジュールは2つの`forward_pre_hooks`を保有しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hCmZe5okxFg",
        "outputId": "bfcb6044-78fb-4dd6-e802-eb66bb98ee1d"
      },
      "source": [
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight_orig', Parameter containing:\n",
            "tensor([[[[ 0.3273,  0.2712, -0.0953],\n",
            "          [ 0.0764, -0.2651, -0.3249],\n",
            "          [ 0.2157, -0.2530,  0.0524]]],\n",
            "\n",
            "\n",
            "        [[[-0.0895,  0.0482,  0.2638],\n",
            "          [-0.2001, -0.1045,  0.2797],\n",
            "          [-0.3160,  0.3137, -0.1420]]],\n",
            "\n",
            "\n",
            "        [[[-0.0745,  0.2557, -0.0125],\n",
            "          [ 0.0688,  0.2205,  0.2299],\n",
            "          [-0.0944,  0.1338, -0.0233]]],\n",
            "\n",
            "\n",
            "        [[[-0.0590, -0.2994, -0.1391],\n",
            "          [-0.0028,  0.2432, -0.2896],\n",
            "          [-0.0729, -0.2522, -0.2231]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1020, -0.0075, -0.1868],\n",
            "          [ 0.2437,  0.0663, -0.0489],\n",
            "          [ 0.2144, -0.1072, -0.3217]]],\n",
            "\n",
            "\n",
            "        [[[-0.0846,  0.1886,  0.0569],\n",
            "          [-0.2620, -0.2686, -0.0275],\n",
            "          [-0.0430,  0.2285, -0.3022]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\n",
            "tensor([ 0.1315,  0.1902,  0.1169, -0.0872, -0.0136,  0.2106], device='cuda:0',\n",
            "       requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo7_hqlckxFg",
        "outputId": "b0ba27de-6ab0-47d5-b8e8-a885f7392479"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight_mask', tensor([[[[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 0.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 0.],\n",
            "          [1., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 1.],\n",
            "          [1., 0., 1.],\n",
            "          [1., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 0.],\n",
            "          [1., 1., 0.],\n",
            "          [1., 1., 0.]]]], device='cuda:0')), ('bias_mask', tensor([1., 1., 0., 0., 0., 1.], device='cuda:0'))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYSVjSQZkxFh",
        "outputId": "59583eec-dea5-45ee-b95b-52ecc9ab534e"
      },
      "source": [
        "print(module.bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.1315, 0.1902, 0.0000, -0.0000, -0.0000, 0.2106], device='cuda:0',\n",
            "       grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L86d94pXkxFh",
        "outputId": "7b5db01b-7ab0-4da9-f899-33064958a4a7"
      },
      "source": [
        "print(module._forward_pre_hooks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([(1, <torch.nn.utils.prune.PruningContainer object at 0x7f937097dba8>), (2, <torch.nn.utils.prune.L1Unstructured object at 0x7f9370978320>)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCtYf1Z1kxFi"
      },
      "source": [
        "## 枝刈りの反復\n",
        "\n",
        "複数回に渡って、モジュール内の同一のパラメータを枝刈りすることも可能です。\n",
        "\n",
        "様々な枝刈りの実行により発生する効果は、様々なマスクを順に適用した場合と等しい結果をもたらします。\n",
        "\n",
        "新たなマスクと古いマスクの組み合わせは、`PruningContainer`の`compute_mask`メソッドによって処理します。\n",
        "\n",
        "例えば今回は、チャネルのL2ノルムに基づいてテンソルの0番目の軸（0番目の軸は、畳み込み層の出力チャネルに対応しており、`conv1`の場合は6つの要素を有します。）に沿った構造的な枝刈りを行い、`module.weight`をさらに枝刈りしたいとします。\n",
        "\n",
        "これらの一連の処理は、`ln_structured`関数の引数に `n=2` と `dim=0` を渡すことによって行えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4xAhqKZkxFi",
        "outputId": "7c8b3646-650a-45eb-e159-4e1ec7470b5f"
      },
      "source": [
        "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
        "\n",
        "# 出力を確認するとわかるように、\n",
        "# 以前のマスクの実行結果を保持した状態で\n",
        "# チャネルの50%（6分の3）に対応する接続がゼロ化されています。\n",
        "\n",
        "print(module.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0764, -0.2651, -0.3249],\n",
            "          [ 0.2157, -0.0000,  0.0524]]],\n",
            "\n",
            "\n",
            "        [[[-0.0895,  0.0482,  0.0000],\n",
            "          [-0.0000, -0.1045,  0.0000],\n",
            "          [-0.3160,  0.3137, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.1886,  0.0000],\n",
            "          [-0.2620, -0.2686, -0.0000],\n",
            "          [-0.0430,  0.2285, -0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBPVRUB2kxFi"
      },
      "source": [
        "この時、対応するフックは `torch.nn.utils.prune.PruningContainer`型になり、`weight`パラメータに適用された枝刈りの履歴を保持します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViZD9b7OkxFj",
        "outputId": "1648b64b-6f28-4219-85e5-fe4898582b17"
      },
      "source": [
        "for hook in module._forward_pre_hooks.values():\n",
        "    if hook._tensor_name == \"weight\":  # 適切なフックを選択\n",
        "        break\n",
        "\n",
        "print(list(hook))  # コンテナ内の枝刈りの履歴"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<torch.nn.utils.prune.RandomUnstructured object at 0x7f937097dfd0>, <torch.nn.utils.prune.RandomUnstructured object at 0x7f937097dcf8>, <torch.nn.utils.prune.LnStructured object at 0x7f9370978710>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UorWILEPkxFj"
      },
      "source": [
        "## 枝刈りされたモデルのシリアル化\n",
        "\n",
        "マスクバッファを含む、関連するすべてのテンソルと枝刈りされたテンソルの演算に使用される元のパラメータは、モデルの`state_dict`に格納されているため、必要に応じて簡単にシリアル化や保存を行うことが可能です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aSGqOEtkxFj",
        "outputId": "03900381-045f-43f0-a7a9-8b9af09cc555"
      },
      "source": [
        "print(model.state_dict().keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V25HSUipkxFk"
      },
      "source": [
        "## 枝刈りの再パラメータ化の除去\n",
        "\n",
        "枝刈りを永続的なものにした上で、`weight_orig` と `weight_mask` の再パラメータ化を除去し、`forward_pre_hook`も除去するには、`torch.nn.utils.prune`の`remove`関数を使用します。\n",
        "\n",
        "なお、これは何も起こらなかったかのように、枝刈りをキャンセルしているわけではない点に注意してください。\n",
        "\n",
        "キャンセルするのではなく、パラメータ`weight`を、枝刈りされたバージョンのモデルのパラメータに再代入することで、枝刈りを適用し、永続的なものにします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlBBNxpykxFk"
      },
      "source": [
        "再パラメータ化の除去前："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg5unBYYkxFk",
        "outputId": "2cca8fc8-97e9-4b03-e707-e9f2efa933e8"
      },
      "source": [
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight_orig', Parameter containing:\n",
            "tensor([[[[ 0.3273,  0.2712, -0.0953],\n",
            "          [ 0.0764, -0.2651, -0.3249],\n",
            "          [ 0.2157, -0.2530,  0.0524]]],\n",
            "\n",
            "\n",
            "        [[[-0.0895,  0.0482,  0.2638],\n",
            "          [-0.2001, -0.1045,  0.2797],\n",
            "          [-0.3160,  0.3137, -0.1420]]],\n",
            "\n",
            "\n",
            "        [[[-0.0745,  0.2557, -0.0125],\n",
            "          [ 0.0688,  0.2205,  0.2299],\n",
            "          [-0.0944,  0.1338, -0.0233]]],\n",
            "\n",
            "\n",
            "        [[[-0.0590, -0.2994, -0.1391],\n",
            "          [-0.0028,  0.2432, -0.2896],\n",
            "          [-0.0729, -0.2522, -0.2231]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1020, -0.0075, -0.1868],\n",
            "          [ 0.2437,  0.0663, -0.0489],\n",
            "          [ 0.2144, -0.1072, -0.3217]]],\n",
            "\n",
            "\n",
            "        [[[-0.0846,  0.1886,  0.0569],\n",
            "          [-0.2620, -0.2686, -0.0275],\n",
            "          [-0.0430,  0.2285, -0.3022]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\n",
            "tensor([ 0.1315,  0.1902,  0.1169, -0.0872, -0.0136,  0.2106], device='cuda:0',\n",
            "       requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g78onFIBkxFl",
        "outputId": "34722f46-a280-490a-e224-f37fb53cd67e"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('weight_mask', tensor([[[[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 0.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 0.],\n",
            "          [1., 1., 0.],\n",
            "          [1., 1., 0.]]]], device='cuda:0')), ('bias_mask', tensor([1., 1., 0., 0., 0., 1.], device='cuda:0'))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2K8jCbKkxFl",
        "outputId": "89b70878-852f-4878-f251-03f9b047ab0d"
      },
      "source": [
        "print(module.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0764, -0.2651, -0.3249],\n",
            "          [ 0.2157, -0.0000,  0.0524]]],\n",
            "\n",
            "\n",
            "        [[[-0.0895,  0.0482,  0.0000],\n",
            "          [-0.0000, -0.1045,  0.0000],\n",
            "          [-0.3160,  0.3137, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.1886,  0.0000],\n",
            "          [-0.2620, -0.2686, -0.0000],\n",
            "          [-0.0430,  0.2285, -0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wbXNiYGkxFm"
      },
      "source": [
        "再パラメータ化の除去後："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snwnYdwnkxFm",
        "outputId": "3b08d8ad-78ad-443c-a4f3-e9b5bd08e5da"
      },
      "source": [
        "prune.remove(module, 'weight')\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bias_orig', Parameter containing:\n",
            "tensor([ 0.1315,  0.1902,  0.1169, -0.0872, -0.0136,  0.2106], device='cuda:0',\n",
            "       requires_grad=True)), ('weight', Parameter containing:\n",
            "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0764, -0.2651, -0.3249],\n",
            "          [ 0.2157, -0.0000,  0.0524]]],\n",
            "\n",
            "\n",
            "        [[[-0.0895,  0.0482,  0.0000],\n",
            "          [-0.0000, -0.1045,  0.0000],\n",
            "          [-0.3160,  0.3137, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.1886,  0.0000],\n",
            "          [-0.2620, -0.2686, -0.0000],\n",
            "          [-0.0430,  0.2285, -0.0000]]]], device='cuda:0', requires_grad=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTPePlyGkxFm",
        "outputId": "6b578495-188b-44cc-f52d-7c2293998e90"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bias_mask', tensor([1., 1., 0., 0., 0., 1.], device='cuda:0'))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeJSfkG5kxFn"
      },
      "source": [
        "## モデル内の複数パラメータの枝刈り \n",
        "\n",
        "本チュートリアルを通して確認できるように、理想的な枝刈りのテクニックと対象のパラメータを指定することで、条件にもよりますが、ネットワーク内の複数のテンソルの枝刈りを簡単に行うことができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWomK4jokxFn",
        "outputId": "ce607c1a-b74f-44a5-e6b5-36f4b0377932"
      },
      "source": [
        "new_model = LeNet()\n",
        "for name, module in new_model.named_modules():\n",
        "    # すべての2次元畳み込み層の接続の20%を枝刈り \n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "    # すべての線形層の接続の40%を枝刈り \n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "\n",
        "print(dict(new_model.named_buffers()).keys())  # すべてのマスクが存在することを確認"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ1HqepnkxFn"
      },
      "source": [
        "## グローバルな枝刈り\n",
        "\n",
        "ここまでの解説では、いわゆる\"ローカル\"な枝刈りの概念のみを説明しました。\n",
        "（日本語訳注：モジュールごとに適用するという意味でローカルです）\n",
        "\n",
        "例えば、各要素の統計量（重みの大きさ，活性化，勾配など）を、そのテンソルの他の要素と排他的に比較することによって，モデルのテンソルを1つずつ枝刈りするような処理です。\n",
        "\n",
        "しかし、一般的かつ、より強力なテクニックは、（例えば）各層の接続の最小20%を除去するのではなく、モデル全体に渡って最小20%の接続を除去することで、モデルを一度に枝刈りすることです。\n",
        "\n",
        "グローバルに枝刈りを行った場合は、恐らく層ごとに異なる枝刈りの割合になります。\n",
        "\n",
        "`torch.nn.utils.prune`の`global_unstructured`関数を使って、グローバルに枝刈りを行う方法を確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lA5bVAwkxFo"
      },
      "source": [
        "model = LeNet()\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S92uUTf2kxFo"
      },
      "source": [
        "これで、枝刈りされた各パラメータ内に生じたスパース性を確認できます。\n",
        "\n",
        "各層において20％のスパース性を確保できているわけではありませんが、グローバルなスパース性は（およそ）20%です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgWpYHvckxFo",
        "outputId": "40eb2869-57f2-4627-eba7-be95c63c4be0"
      },
      "source": [
        "print(\n",
        "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv1.weight == 0))\n",
        "        / float(model.conv1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv2.weight == 0))\n",
        "        / float(model.conv2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc1.weight == 0))\n",
        "        / float(model.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc2.weight == 0))\n",
        "        / float(model.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc3.weight == 0))\n",
        "        / float(model.fc3.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            torch.sum(model.conv1.weight == 0)\n",
        "            + torch.sum(model.conv2.weight == 0)\n",
        "            + torch.sum(model.fc1.weight == 0)\n",
        "            + torch.sum(model.fc2.weight == 0)\n",
        "            + torch.sum(model.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            model.conv1.weight.nelement()\n",
        "            + model.conv2.weight.nelement()\n",
        "            + model.fc1.weight.nelement()\n",
        "            + model.fc2.weight.nelement()\n",
        "            + model.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sparsity in conv1.weight: 1.85%\n",
            "Sparsity in conv2.weight: 7.87%\n",
            "Sparsity in fc1.weight: 21.95%\n",
            "Sparsity in fc2.weight: 12.66%\n",
            "Sparsity in fc3.weight: 10.36%\n",
            "Global sparsity: 20.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEYrR6W_kxFp"
      },
      "source": [
        "## オリジナルの枝刈りの関数を用いた`torch.nn.utils.prune`の拡張\n",
        "\n",
        "独自の枝刈りの関数を実装するには、他のすべての枝刈りのメソッドが行っている方法と同様、`BasePruningMethod`の基底クラスをサブクラス化することで`nn.utils.prune`を拡張できます。\n",
        "\n",
        "基底クラスには、`__call__`、`apply_mask`、`apply`、`prune`、そして`remove` といったメソッドが実装されています。\n",
        "\n",
        "いくつかの特殊なケースを除いては、新しい枝刈りのテクニックのためにこれらのメソッドを再実装する必要は生じません。\n",
        "\n",
        "しかし、`__init__`（コンストラクター）と`compute_mask`（枝刈りのテクニックのロジックに応じた、所与のテンソルに対するマスクを処理する方策）は実装する必要があります。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0COwvc2vC7se"
      },
      "source": [
        "さらに、どのタイプのテクニックの実装にするかを指定する必要があります（`global`、`structured`、そして`unstructured`の選択肢がサポートされています。）。\r\n",
        "\r\n",
        "これは、枝刈りが繰り返し適用された場合に、どのようにマスクを組み合わせるか判断するために必要な準備になります。\r\n",
        "\r\n",
        "言い換えれば、既に枝刈りされたパラメータを枝刈りする場合、その時点で使用されている枝刈りのテクニックはパラメータの枝刈りされていない部分に作用することが期待されています。\r\n",
        "\r\n",
        "`PRUNING_TYPE`を指定することで、（枝刈りマスクの反復適用を扱う）`PruningContainer`が枝刈りするパラメータの断面を適切に識別できるようにします。\r\n",
        "\r\n",
        "例えば、テンソル内で一つおきに枝刈りを行うテクニックを実装したいとしましょう（または、テンソルが既に枝刈りされていた場合は、そのテンソルの残りの枝刈りされていない部分）。\r\n",
        "\r\n",
        "この場合、`PRUNING_TYPE='unstructured'` とします。\r\n",
        "\r\n",
        "なぜならば枝刈りの対象が、ユニット/チャネル全体 に対して(`'structured'`)ではなく、異なるパラメータに渡る(`'global'`)なわけでもなく、層内の個別の接続に対して行われるためです。\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwoXwVtBkxFq"
      },
      "source": [
        "class FooBarPruningMethod(prune.BasePruningMethod):\n",
        "    \"\"\"\n",
        "    テンソル内で一つおきに枝刈りを行う\n",
        "    \"\"\"\n",
        "    PRUNING_TYPE = 'unstructured'\n",
        "\n",
        "    def compute_mask(self, t, default_mask):\n",
        "        mask = default_mask.clone()\n",
        "        mask.view(-1)[::2] = 0 \n",
        "        return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXZuK5hMkxFr"
      },
      "source": [
        "では、`nn.Module`内のパラメータにこれを適用するために、メソッドをインスタンス化し、そのメソッドを適用する簡単な関数を準備しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxjl7OuJkxFr"
      },
      "source": [
        "def foobar_unstructured(module, name):\n",
        "    \"\"\"\n",
        "    テンソル内のエントリを1つおきに除去することで、\n",
        "    `module`内の`name`というパラメータに対応するテンソルを枝刈りする。\n",
        "    以下の要領でin-placeにモジュールを変更する（そして変更されたモジュールを返す）。\n",
        "    1) 枝刈りのメソッドによって、`name`パラメータに適用されたバイナリのマスクに対応する\n",
        "    `name+'_mask'` という名前付きバッファを加える。\n",
        "    `name`パラメータは枝刈りされたバージョンに置換される一方で、\n",
        "    元の（枝刈りされていない）パラメータは、`name+'_orig'`という名前の新しいパラメータに格納される。\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): 枝刈りの対象となるテンソルを含むモジュール\n",
        "        name (string): 枝刈りが作用する対象となる`module`内のパラメータ名\n",
        "\n",
        "    Returns:\n",
        "        module (nn.Module): 変更（例：枝刈り）されたバージョンの入力モジュール\n",
        "    \n",
        "    Examples:\n",
        "        >>> m = nn.Linear(3, 4)\n",
        "        >>> foobar_unstructured(m, name='bias')\n",
        "    \"\"\"\n",
        "    FooBarPruningMethod.apply(module, name)\n",
        "    return module"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQHVtydkkxFs"
      },
      "source": [
        "試してみましょう！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80Qib4NGkxFs",
        "outputId": "70f1286e-a427-40fe-e12f-dd7104178059"
      },
      "source": [
        "model = LeNet()\n",
        "foobar_unstructured(model.fc3, name='bias')\n",
        "\n",
        "print(model.fc3.bias_mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_LY9qnVDIhz"
      },
      "source": [
        "以上。"
      ]
    }
  ]
}