{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "6_2_model_parallel_tutorial_jp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcFZONQe1Vb4"
      },
      "source": [
        "# 「シングルマシン環境におけるモデル並列訓練」\n",
        "\n",
        "【原題】Single-Machine Model Parallel Best Practices\n",
        "\n",
        "【原著】[Shen Li](https://mrshenli.github.io/)\n",
        "\n",
        "【元URL】https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html\n",
        "\n",
        "【翻訳】電通国際情報サービスISID HCM事業部　櫻井 亮佑\n",
        "\n",
        "【日付】2020年11月14日\n",
        "\n",
        "【チュトーリアル概要】\n",
        "\n",
        "モデル並列は分散訓練のテクニックで広く使用されています。\n",
        "\n",
        "1つ前のチュートリアル（日本語6_1）では、複数のGPU上でニューラルネットワークを訓練する際に、 [DataParallel](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html) を使用する方法を紹介しました。\n",
        "\n",
        "この機能は同一のモデルをすべてのGPU上に複製し、各GPUが入力データの異なる部分を利用します。\n",
        "\n",
        "データ並列化は訓練プロセスを大幅に高速化出来ますが、モデルが大きすぎて単一のGPUに収まりきらないようなユースケースではうまく機能しません。\n",
        "\n",
        "<br>\n",
        "\n",
        "本チュートリアルでは、**モデル並列化**を使ってこの問題に対処する方法を説明します。\n",
        "\n",
        "モデル並列化では、`DataParallel` のようにモデル全体を各GPU上に複製するのではなく、単一のモデルを異なる複数のGPU上に分割します。\n",
        "\n",
        "（モデル`m`が10層を有しているとすれば、`DataParallel`の場合には、各GPUはこれら10層の複製を保持することになりますが、2つのGPU上でモデル並列化を行った場合には、各GPUが5層を管理します）\n",
        "\n",
        "モデル並列のアイデアの基本は、モデル内の異なるサブネットワークをそれぞれ異なるデバイス上に配置し、配置に応じた `forward` メソッドを実装してデバイス間で中間出力を移動させることです。\n",
        "\n",
        "モデルの一部だけが個々のデバイス上で動作するため、デバイスのセットとしてはより大きなモデルをまとめて提供することができます。\n",
        "\n",
        "なお、本投稿では、巨大なモデルを構築し、限られた数のGPUにそれらのモデルを詰め込むことはせず、代わりにモデル並列のアイデアに焦点を当てます。\n",
        "\n",
        "実世界のアプリケーションにモデル並列のアイデアを適用するかは読者の皆様に委ねます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNvtCirh1VcF"
      },
      "source": [
        "**注意**\n",
        "\n",
        "モデルが複数サーバーを横断する、分散モデル並列訓練については、[分散RPCフレームワーク入門](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html) のサンプル例や詳細を参照してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBU0fz2p1VcG"
      },
      "source": [
        "## 基本的な使用方法\n",
        "\n",
        "まずは2つの線形層から構成される、簡単なトイモデルから始めましょう。\n",
        "\n",
        "このモデルを2つのGPU上で実行するには、各線形層を異なるGPU上に配置し、配置した層に合うように入力と中間の出力をGPU上に移動するだけです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU3kQT_j1VcC"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhPiqym41VcG"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class ToyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ToyModel, self).__init__()\n",
        "        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.net2 = torch.nn.Linear(10, 5).to('cuda:1')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.net1(x.to('cuda:0')))\n",
        "        return self.net2(x.to('cuda:1'))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fndbl_z71VcH"
      },
      "source": [
        "上記の `ToyModel` は、適切なデバイス上に線形層とテンソルを配置する4つの `to(device)` の呼び出しを除いては、単一のGPU 上での実装方法に非常に似ている点に留意してください。\n",
        "\n",
        "そして、モデルの実装で、通常から変更が必要になる部分も、この点のみです。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg9NdAuyU_VA"
      },
      "source": [
        "なお、モデルが単一のGPU上に存在する場合と同様、`backward()` と `torch.optim` は自動的に勾配の情報を管理してくれます。\r\n",
        "\r\n",
        "\r\n",
        "ただし、損失関数を呼び出す際には、ラベルと出力が同じデバイスに存在する必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-tQI69C1VcH"
      },
      "source": [
        "model = ToyModel()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "outputs = model(torch.randn(20, 10))\n",
        "labels = torch.randn(20, 5).to('cuda:1')\n",
        "loss_fn(outputs, labels).backward()\n",
        "optimizer.step()\n",
        "\n",
        "# 日本語訳注：追記\n",
        "# ToyModelの最後のforwardの最終層は'cuda:1'で処理されるので、labelsも'cuda:1'に集めています"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu2hiBMt1VcI"
      },
      "source": [
        "## 既存モジュールへのモデル並列の適用方法\n",
        "\n",
        "コードを数行変更するだけで、既存のシングルGPU用のモジュールをマルチGPUで実行することも可能です。\n",
        "\n",
        "下記のコードでは、 `torchvision.models.resnet50()` を、2つのGPU上に処理できるように分解する方法を解説しています。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLAwEOQpVy1T"
      },
      "source": [
        "内容としては、既存の `ResNet` モジュールを継承し、モデルの構築中に2つのGPU上に層を分割しています。\r\n",
        "\r\n",
        "そして、 `forward` メソッドをオーバーライドし、中間出力を適宜移動することで2つのサブネットワークを結合させています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kB5F8Or1VcJ"
      },
      "source": [
        "from torchvision.models.resnet import ResNet, Bottleneck\n",
        "\n",
        "num_classes = 1000\n",
        "\n",
        "\n",
        "class ModelParallelResNet50(ResNet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(ModelParallelResNet50, self).__init__(\n",
        "            Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)\n",
        "\n",
        "        self.seq1 = nn.Sequential(\n",
        "            self.conv1,\n",
        "            self.bn1,\n",
        "            self.relu,\n",
        "            self.maxpool,\n",
        "\n",
        "            self.layer1,\n",
        "            self.layer2\n",
        "        ).to('cuda:0')\n",
        "\n",
        "        self.seq2 = nn.Sequential(\n",
        "            self.layer3,\n",
        "            self.layer4,\n",
        "            self.avgpool,\n",
        "        ).to('cuda:1')\n",
        "\n",
        "        self.fc.to('cuda:1')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.seq2(self.seq1(x).to('cuda:1'))\n",
        "        return self.fc(x.view(x.size(0), -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3vxBMhG1VcJ"
      },
      "source": [
        "上記のような実装は、モデルが大きすぎて単一のGPUに収まらないケースにも対応できます。\n",
        "\n",
        "ですが、既にお気付きかもしれませんが、上記では、モデルが1つのGPUに入る大きさの場合には、シングルGPUで実行する場合よりも、実行が遅くなります。\n",
        "\n",
        "これは、どの時点においても2つのGPUの内1つのGPUしか稼働せず、もう一方は何もせずにただ存在しているアイドル状態のためです。\n",
        "\n",
        "特に、`layer2` と `layer3` の間では、中間出力を `cuda:0` から `cuda:1` にコピーする必要があるため、性能がさらに低下します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hkQSGMG1VcK"
      },
      "source": [
        "実行時間に関して定量的な観測をするために一つ実験を行ってみましょう。\n",
        "\n",
        "この実験では、ランダムな入力とラベルを用いて、 `ModelParallelResNet50` と既存の `torchvision.models.resnet50()` の訓練をします。\n",
        "\n",
        "訓練を行った後にモデルが何か意味のある予測を出力するわけではありませんが、実行時間に関する、定量的な証拠を求めることができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcQxyvlT1VcK"
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "num_batches = 3\n",
        "batch_size = 120\n",
        "image_w = 128\n",
        "image_h = 128\n",
        "\n",
        "\n",
        "def train(model):\n",
        "    model.train(True)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "    one_hot_indices = torch.LongTensor(batch_size) \\\n",
        "                           .random_(0, num_classes) \\\n",
        "                           .view(batch_size, 1)\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        # ランダムな入力値とラベルを生成\n",
        "        inputs = torch.randn(batch_size, 3, image_w, image_h)\n",
        "        labels = torch.zeros(batch_size, num_classes) \\\n",
        "                      .scatter_(1, one_hot_indices, 1)\n",
        "\n",
        "        # フォワードパスの実行\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to('cuda:0'))\n",
        "\n",
        "        # バックワードパスの実行\n",
        "        labels = labels.to(outputs.device)\n",
        "        loss_fn(outputs, labels).backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF3qbkI81VcL"
      },
      "source": [
        "上記の `train(model)` メソッドは、損失関数として `nn.MSELoss` を、最適化関数として`optim.SGD` を使用しています。\n",
        "\n",
        "また、処理内容自体は、 `128 x 128` の画像に対する訓練を想定し、これらの画像が1つのミニバッチあたり120枚含まれます。\n",
        "\n",
        "このようなミニバッチが3つあるデータセットです。\n",
        "\n",
        "最後に `timeit` を使用してから `train(model)` メソッドを10回実行し、標準偏差とともに実行時間をプロットします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKP2cRP61VcL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('Agg')\n",
        "import numpy as np\n",
        "import timeit\n",
        "\n",
        "num_repeat = 10\n",
        "\n",
        "stmt = \"train(model)\"\n",
        "\n",
        "setup = \"model = ModelParallelResNet50()\"\n",
        "# globals引数は、Python3系でのみ利用可能です。\n",
        "# Python2系では、以下の記述をしてください。\n",
        "# import __builtin__\n",
        "# __builtin__.__dict__.update(locals())\n",
        "mp_run_times = timeit.repeat(\n",
        "    stmt, setup, number=1, repeat=num_repeat, globals=globals())\n",
        "mp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)\n",
        "\n",
        "setup = \"import torchvision.models as models;\" + \\\n",
        "        \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\"\n",
        "rn_run_times = timeit.repeat(\n",
        "    stmt, setup, number=1, repeat=num_repeat, globals=globals())\n",
        "rn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)\n",
        "\n",
        "\n",
        "def plot(means, stds, labels, fig_name):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar(np.arange(len(means)), means, yerr=stds,\n",
        "           align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n",
        "    ax.set_ylabel('ResNet50 Execution Time (Second)')\n",
        "    ax.set_xticks(np.arange(len(means)))\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.yaxis.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_name)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "plot([mp_mean, rn_mean],\n",
        "     [mp_std, rn_std],\n",
        "     ['Model Parallel', 'Single GPU'],\n",
        "     'mp_vs_rn.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbBFLADvIkDt"
      },
      "source": [
        "<img src=\"https://pytorch.org/tutorials/_images/mp_vs_rn.png\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOW9HhE61VcM"
      },
      "source": [
        "結果は、モデル並列の実装モデルの実行時間は `4.02/3.75-1=7%` と、7%長い時間がかかり、シングルGPUの実装モデルよりも遅い結果となりました。\n",
        "\n",
        "つまり、GPU間でテンソルをコピーするやり取りに、およそ7%のオーバーヘッドが発生していると結論付けることができます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11UTzEuKtMlQ"
      },
      "source": [
        "ですが、実行を通して2つの内の1つのGPUがアイドル状態であることが分かっているので、改善の余地はあります。\r\n",
        "\r\n",
        "一つの選択肢は、ミニバッチのパイプラインを分割し、一つの分割されたパイプラインが第二のサブネットワークに到達したときに、次の分割されたパイプラインを第一のサブネットワークに供給できるようにすることです。\r\n",
        "\r\n",
        "\r\n",
        "これにより、連続した2つの分割されたパイプラインを2つのGPU上で同時に実行できるようになります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBamXKn91VcM"
      },
      "source": [
        "## 入力のパイプライン化による高速化\n",
        "\n",
        "次の実験では、120枚の画像を含む各バッチをさらに20枚の画像毎に分割します。\n",
        "\n",
        "なお、PyTorchではCUDAの操作を非同期で開始するため、並列性を実現するために、わざわざマルチスレッドを生成するための実装追加は必要ありません。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHUBkVHf1VcN"
      },
      "source": [
        "class PipelineParallelResNet50(ModelParallelResNet50):\n",
        "    def __init__(self, split_size=20, *args, **kwargs):\n",
        "        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n",
        "        self.split_size = split_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        splits = iter(x.split(self.split_size, dim=0))\n",
        "        s_next = next(splits)\n",
        "        s_prev = self.seq1(s_next).to('cuda:1')\n",
        "        ret = []\n",
        "\n",
        "        for s_next in splits:\n",
        "            # A. s_prev は cuda:1 上で実行されます。\n",
        "            s_prev = self.seq2(s_prev)\n",
        "            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n",
        "\n",
        "            # B. s_next はAと同時に cuda:0 上で実行されます。\n",
        "            s_prev = self.seq1(s_next).to('cuda:1')\n",
        "\n",
        "        s_prev = self.seq2(s_prev)\n",
        "        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n",
        "\n",
        "        return torch.cat(ret)\n",
        "\n",
        "\n",
        "setup = \"model = PipelineParallelResNet50()\"\n",
        "pp_run_times = timeit.repeat(\n",
        "    stmt, setup, number=1, repeat=num_repeat, globals=globals())\n",
        "pp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n",
        "\n",
        "plot([mp_mean, rn_mean, pp_mean],\n",
        "     [mp_std, rn_std, pp_std],\n",
        "     ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'],\n",
        "     'mp_vs_rn_vs_pp.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxM3mJLR1VcN"
      },
      "source": [
        "デバイス間のテンソルのコピー操作は、コピー元のデバイスとコピー先のデバイスの間のストリーム上で同期化されることに気を付けてください。\n",
        "\n",
        "もし複数のストリームを作成した場合には、コピー操作が適切に同期化されるようにする必要があります。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3b_y2OtuA3N"
      },
      "source": [
        "コピー操作が終了する前にコピー元のテンソルを書き込んだり、コピー先のテンソルを読み書きしたりすると、未定義の動作になることがあります。\r\n",
        "\r\n",
        "上記の実装においては、コピー元のデバイス、コピー先のデバイスの双方でデフォルトのストリームを使用しているだけなので、追加で強制的に同期を行う必要はありません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgsWyeABKxnP"
      },
      "source": [
        "<img src=\"https://pytorch.org/tutorials/_images/mp_vs_rn_vs_pp.png\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQPkPErY1VcO"
      },
      "source": [
        "実験の結果から、並列モデルのResNet50への入力をパイプライン化することで、約 `3.75/2.51-1=49%` と、訓練プロセスが約50%高速化することがわかります。\n",
        "\n",
        "2GPUなので、理想である100％の高速化（処理時間半分）からは、まだかけ離れています。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmiKa3a2ujpg"
      },
      "source": [
        "新しいパラメーターである `split_sizes` をパイプライン並列の実装に導入しましたが、このパラメーターが訓練時間全体にどのように影響を与えるかが不透明です。\r\n",
        "\r\n",
        "直感的に表現すれば、`split_size` に小さい値を設定した場合は多くの小さなCUDAカーネルが起動しますが、大きい値を設定した場合は、最初と最後の分割の間で比較的長いアイドル時間が生じるようになります。\r\n",
        "\r\n",
        "どちらも最適な状態ではありません。\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "しかし、この実験には、最適な `split_size` の設定値があるかもしれません。\r\n",
        "\r\n",
        "異なる `split_size` の設定値をいくつか試して実験を行い、最適な設定値を探してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky8IyUea1VcP"
      },
      "source": [
        "means = []\n",
        "stds = []\n",
        "split_sizes = [1, 3, 5, 8, 10, 12, 20, 40, 60]\n",
        "\n",
        "for split_size in split_sizes:\n",
        "    setup = \"model = PipelineParallelResNet50(split_size=%d)\" % split_size\n",
        "    pp_run_times = timeit.repeat(\n",
        "        stmt, setup, number=1, repeat=num_repeat, globals=globals())\n",
        "    means.append(np.mean(pp_run_times))\n",
        "    stds.append(np.std(pp_run_times))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(split_sizes, means)\n",
        "ax.errorbar(split_sizes, means, yerr=stds, ecolor='red', fmt='ro')\n",
        "ax.set_ylabel('ResNet50 Execution Time (Second)')\n",
        "ax.set_xlabel('Pipeline Split Size')\n",
        "ax.set_xticks(split_sizes)\n",
        "ax.yaxis.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"split_size_tradeoff.png\")\n",
        "plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuLRx7heMAZJ"
      },
      "source": [
        "<img src=\"https://pytorch.org/tutorials/_images/split_size_tradeoff.png\"></img>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLi_Oc7p1VcP"
      },
      "source": [
        "実験の結果、`split_size` には12を設定することで最速の訓練速度を実現可能であり、`3.75/2.43-1=54%` と、高速化が期待できることがわかりました。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjA7wldIu2iS"
      },
      "source": [
        "しかし、訓練プロセスをさらに加速させる機会はまだあります。\r\n",
        "\r\n",
        "例えば、現在 `cuda:0` 上のすべての操作はデフォルトのストリーム上で行われています。\r\n",
        "\r\n",
        "これは、次の分割の演算が前の分割のコピー操作と重ならないで行われていることを意味します。\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "しかし、前の分割と次の分割のテンソルは異なっているため、ある分割の演算が他の分割のコピー処理と重なっても問題はありません。\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_tpjm3vvaFp"
      },
      "source": [
        "このように、前後の分割の処理が一部重なるように処理を流すには、双方のGPU上で複数のストリームを使用する実装をし、異なるサブネットワークの構造で異なるストリーム管理の方策を採る必要があります。\r\n",
        "\r\n",
        "なお、一般的なマルチストリームの手法のすべてが、今回のモデル並列のユースケースで動作するものではないので、これ以上は本チュートリアルでは取り扱いません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOn_Ba4k1VcQ"
      },
      "source": [
        "**注意:**<br>\n",
        "本チュートリアルではいくつかの性能指標を示しています。\n",
        "\n",
        "掲載コードを自身のマシン上で実行する際には、異なる数値が表示されるかもしれませんが、これは実行結果がハードウェアやソフトウェアに依存しているためです。\n",
        "\n",
        "自身の環境で最良のパフォーマンスを得る上での適切なアプローチは、まず曲線を生成して最適な分割サイズを把握し、その分割サイズをパイプラインの入力に使用することです。"
      ]
    }
  ]
}