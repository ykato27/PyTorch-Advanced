{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_4_writing_distributed_applications_with_pytorch_jp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVMAdAuSnKIE"
      },
      "source": [
        "# 「PyTorchで実装する分散アプリケーション」\n",
        "\n",
        "【原題】Writing Distributed Applications with PyTorch \n",
        "\n",
        "【原著】[Séb Arnold](https://seba1511.com/)\n",
        "\n",
        "【元URL】https://pytorch.org/tutorials/intermediate/dist_tuto.html\n",
        "\n",
        "【翻訳】電通国際情報サービスISID HCM事業部　櫻井 亮佑\n",
        "\n",
        "【日付】2020年11月21日\n",
        "\n",
        "【チュトーリアル概要】\n",
        "\n",
        "前提知識\n",
        "- [PyTorch Distributedについて](https://pytorch.org/tutorials/beginner/dist_overview.html)(日本語チュートリアル6_1)\n",
        "\n",
        "<br>\n",
        "\n",
        "本チュートリアルでは、PyTorchの分散パッケージについて紹介します。\n",
        "\n",
        "具体的には、分散設定のセットアップ方法を確認し、非同期処理における通信方法（のうちの3つ）を実際に実装してみます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQh3krgQn1vO"
      },
      "source": [
        "## セットアップ\n",
        "\n",
        "PyTorchに含まれている分散パッケージ（例えば、`torch.distributed`）を利用することで、研究者やエンジニアは、プロセスやマシンのクラスタ間での計算を簡単に並列化できます。\n",
        "\n",
        "\n",
        "並列化するには、メッセージパッシングセマンティクスを活用し、各プロセスが他のプロセスとデータ通信できるようにします。\n",
        "\n",
        "マルチプロセスパッケージ（`torch.multiprocessing`）とは違い、プロセスは異なる通信バックエンドを使用することが可能であり、同一マシン内という条件に制限されることはありません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9ZT8UxJn4jb"
      },
      "source": [
        "分散アプリケーションを始めるには、同時に複数のプロセスを実行するスキルが必要になります。\n",
        "\n",
        "コンピューティングマシン-クラスタに対するアクセス権を保有している場合は、ローカルのシステム管理者に確認するか、お好みの調整ツール（例えば、[pdsh](https://linux.die.net/man/1/pdsh)、[clustershell](https://cea-hpc.github.io/clustershell/)、または[その他](https://slurm.schedmd.com/)）を使ってください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcwLq4h4KxTu"
      },
      "source": [
        "\r\n",
        "本チュートリアルの目的は、これから実装するテンプレートを用いて、単一のマシン上で処理をマルチプロセスに分岐させることです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WmbWQm7nj0L"
      },
      "source": [
        "\"\"\"run.py:\"\"\"\n",
        "#!/usr/bin/env python\n",
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.multiprocessing import Process\n",
        "\n",
        "def run(rank, size):\n",
        "    \"\"\" 後ほど、分散する関数を実装します。\"\"\"\n",
        "    pass\n",
        "\n",
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" 分散環境を初期化します。 \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn(rank, size)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    size = 2\n",
        "    processes = []\n",
        "    for rank in range(size):\n",
        "        p = Process(target=init_process, args=(rank, size, run))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "        p.join()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETyS9La6n_aO"
      },
      "source": [
        "上記のスクリプトは、それぞれの分散環境をセットアップする2つのプロセスを生成します。\n",
        "\n",
        "\n",
        "これによりプロセスグループ（`dist.init_process_group`）を初期化し、最終的には与えられた `run` 関数を実行します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ4XyG8FoBbt"
      },
      "source": [
        "`init_process` 関数を確認しましょう。\n",
        "同一のipアドレスとポートを使うことで、マスターを通して各プロセスが同期した動作ができるようにしています。\n",
        "\n",
        "なお、上記のコードでは `gloo` バックエンドを使用しましたが、その他のバックエンドも利用可能である点に留意してください（セクション「発展的なトピック」を参照）。\n",
        "\n",
        "\n",
        "本チュートリアルの最後には、`dist.init_process_group` で起こるマジックを紹介しますが、本質的には、各マシンの情報を共有することでプロセスが相互に通信を行えるようにしているだけです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCUaHcADoDgO"
      },
      "source": [
        "## ポイントツーポイント通信"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKeqgbAXoG-m"
      },
      "source": [
        "<img src=\"https://pytorch.org/tutorials/_images/send_recv.png\"></src><br>\n",
        "送受信"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Zgm5nooUw5"
      },
      "source": [
        "あるプロセスから別のプロセスへの移動は、ポイントツーポイント通信と呼ばれています。\n",
        "\n",
        "ポイントツーポイント通信は、`send`関数と `recv`関数、またはそれらと同様の処理を即時に行う `isend`関数と `irecv`関数によって実現されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrM4BWKmoLTw"
      },
      "source": [
        "\"\"\" ポイントツーポイント通信 \"\"\"\n",
        "\n",
        "def run(rank, size):\n",
        "    tensor = torch.zeros(1)\n",
        "    if rank == 0:\n",
        "        tensor += 1\n",
        "        # プロセス1にテンソルを送信\n",
        "        dist.send(tensor=tensor, dst=1)\n",
        "    else:\n",
        "        # プロセス0からテンソルを受信\n",
        "        dist.recv(tensor=tensor, src=0)\n",
        "    print('Rank ', rank, ' has data ', tensor[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZyeTEbiobQQ"
      },
      "source": [
        "上記の実装例では、2つのプロセスが値ゼロのテンソルから始まっており、プロセス0がテンソルをインクリメントしてプロセス1に送信しているため、最終的には両方のプロセスに存在するテンソルが1.0の状態で終了します。\n",
        "\n",
        "なお、受信するデータを格納するために、プロセス1にはメモリを配分する必要がある点に留意してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpbBfPdgodLb"
      },
      "source": [
        "また、`send` / `recv` は処理をブロックするため、通信が完了するまで両方のプロセスが止まる点にも留意してください。\n",
        "\n",
        "一方で、即時で通信を行う `isend`関数と `irecv`関数は、処理をブロックしません。\n",
        "\n",
        "この場合、スクリプトは処理の実行を継続し、メソッドは `wait()` を実行できる `Work` オブジェクトを返します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W0QCu0woe2d"
      },
      "source": [
        "\"\"\" ブロックを行わないポイントツーポイント通信 \"\"\"\n",
        "\n",
        "def run(rank, size):\n",
        "    tensor = torch.zeros(1)\n",
        "    req = None\n",
        "    if rank == 0:\n",
        "        tensor += 1\n",
        "        # テンソルをプロセス1に送信\n",
        "        req = dist.isend(tensor=tensor, dst=1)\n",
        "        print('Rank 0 started sending')\n",
        "    else:\n",
        "        # プロセス0からテンソルを受信\n",
        "        req = dist.irecv(tensor=tensor, src=0)\n",
        "        print('Rank 1 started receiving')\n",
        "    req.wait()\n",
        "    print('Rank ', rank, ' has data ', tensor[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgzNEQnCoiA1"
      },
      "source": [
        "即時に処理を行う関数を使用する場合、送受信されたテンソルの使用方法に気を付ける必要があります。\n",
        "\n",
        "具体的には、データがいつ他のプロセスへ通信されるかについて私たちはわからないため、`req.wait()` の処理が完了する前に送信されたテンソルに手を加えたり、受信したテンソルにアクセスするべきではありません。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_2ql1Q9MyH6"
      },
      "source": [
        "言い換えれば以下のようになります。\r\n",
        "\r\n",
        "- `dist.isend()` の後に `tensor` に書き込みを行うと、未定義のエラーが発生します。\r\n",
        "- `dist.irecv()` の後に `tensor` から読み込みを行うと、未定義のエラーが発生します。\r\n",
        "\r\n",
        "しかし、`req.wait()` の実行が完了した後であれば、通信が発生・完了し、`tensor[0]` に格納されている値が1.0であることが保証されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr9wueZKoxXn"
      },
      "source": [
        "ポイントツーポイント通信は、プロセスの通信を細部まで制御したい場合に便利です。\n",
        "\n",
        "また、[BaiduのDeepSpeech](https://github.com/baidu-research/baidu-allreduce) や [Facebookの大規模実験](https://research.fb.com/publications/imagenet1kin1h/) で使用されているような、大規模なアルゴリズムを実装する際にも使用することができます（セクション「独自のRing-Allreduceの実装」を参照）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojg3JbIao42g"
      },
      "source": [
        "## 集合通信"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuA2dcEuo6Z_"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "<td>\n",
        "<img src=\"https://pytorch.org/tutorials/_images/scatter.png\"></img>\n",
        "<p>Scatter</p>\n",
        "</td>\n",
        "<td>\n",
        "<img src=\"https://pytorch.org/tutorials/_images/gather.png\"></img>\n",
        "<p>Gather</p>\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>\n",
        "<img src=\"https://pytorch.org/tutorials/_images/reduce.png\"></img>\n",
        "<p>Reduce</p>\n",
        "</td>\n",
        "<td>\n",
        "<img src=\"https://pytorch.org/tutorials/_images/all_reduce.png\"></img>\n",
        "<p>All-Reduce</p>\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>\n",
        "<img src=\"https://pytorch.org/tutorials/_images/broadcast.png\"></img>\n",
        "<p>Broadcast</p>\n",
        "</td>\n",
        "<td>\n",
        "<img src=\"https://pytorch.org/tutorials/_images/all_gather.png\"></img>\n",
        "<p>All-Gather</p>\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5usvQWcq57r"
      },
      "source": [
        "ポイントツーポイント通信とは異なり、集合通信は**グループ内**の全てのプロセス間でのコミュニケーションパターンを可能にします。\n",
        "\n",
        "なお、ここでのグループとは、すべてのプロセスのサブセットを指しています。\n",
        "グループを作成するには、`dist.new_group(group)` にランクの配列を与えます。\n",
        "\n",
        "デフォルトでは、集合通信は**ワールド**とも表現される全てのプロセス上で実行されます。\n",
        "\n",
        "例えば、すべてのプロセスにおけるテンソルの合計を得るには、`dist.all_reduce(tensor, op, group)` の集合通信を使います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_GqqsPxq9tS"
      },
      "source": [
        "\"\"\" All-Reduce の例 \"\"\"\n",
        "def run(rank, size):\n",
        "    \"\"\" 単純なポイントツーポイント通信 \"\"\"\n",
        "    group = dist.new_group([0, 1])\n",
        "    tensor = torch.ones(1)\n",
        "    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n",
        "    print('Rank ', rank, ' has data ', tensor[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-MMSW_kq8bD"
      },
      "source": [
        "グループ内のすべてのテンソルの合計を得たいため、reduce演算子として `dist.reduce_op.SUM` を使います。\n",
        "一般的に、数学的に可換性のある演算は、単一の演算子として使用することが可能です。<br>\n",
        "なお、PyTorchは4つの演算子を用意しており、すべて要素単位のレベルで動作します。\n",
        "- `dist.reduce_op.SUM`,\n",
        "- `dist.reduce_op.PRODUCT`,\n",
        "- `dist.reduce_op.MAX`,\n",
        "- `dist.reduce_op.MIN`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONq0_uFYrIjV"
      },
      "source": [
        "`dist.all_reduce(tensor, op, group)` に加えて、現在PyTorchには、合計6つの集合通信の関数が実装されています（先ほどの図の通りです）。\n",
        "\n",
        "- `dist.scatter(tensor, src, scatter_list, group)`: i番目のテンソル（`scatter_list[i]`）を、i番目のプロセスにコピーします。\n",
        "- `dist.gather(tensor, dst, gather_list, group)`: `dst` 内のすべてのプロセスから `tensor` をコピーします。\n",
        "- `dist.reduce(tensor, dst, op, group)`: opをすべてのテンソルに適用し、結果を `dst` に格納します。\n",
        "- `dist.all_reduce(tensor, op, group)`: reduceと同様の処理を行いますが、すべてのプロセスに結果を格納します。\n",
        "- `dist.broadcast(tensor, src, group)`: srcから他のすべてのプロセスに対して `tensor` をコピーします。\n",
        "- `dist.all_gather(tensor_list, tensor, group)`: すべてのプロセス上で、すべてのプロセスから  `tensor_list` に `tensor` をコピーします。  \n",
        "\n",
        "そして、\n",
        "- `dist.barrier(group)`: 各プロセスがこの関数に到達するまで、グループ内のすべてのプロセスをブロックします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBRnaelRrSXN"
      },
      "source": [
        "## 分散訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhMPHPwXrXI7"
      },
      "source": [
        "**注意:** 本セクションのサンプルスクリプトは、[こちらのGitHubのリポジトリ](https://github.com/seba-1511/dist_tuto.pth/)で確認できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5NllhIrrY9m"
      },
      "source": [
        "分散モジュールの仕組みを理解したところで、実際に分散モジュールを使った実装をしてみましょう。\n",
        "\n",
        "目標は、[DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel) の機能を再現することです。\n",
        "\n",
        "もちろん、これは教育目的のサンプル例です。\n",
        "\n",
        "実際の状況では十分にテストされ、最適化された公式の[DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)を使用するべきです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAiDbCwfra28"
      },
      "source": [
        "簡単に表せば、これから確率的勾配降下法の分散版を実装したいと考えています。\n",
        "\n",
        "スクリプトでは、データのバッチを使って、すべてのプロセスにモデルの勾配を演算させ、その後各プロセスで算出された勾配を平均化します。\n",
        "\n",
        "なお、プロセスの数を変えた際に似たような収束結果を得られるようにするためには、初めにデータセットを分割する必要があります。\n",
        "\n",
        "（下記に実装を記載するクラスの代わりに、[tnt.dataset.SplitDataset](https://github.com/pytorch/tnt/blob/master/torchnet/dataset/splitdataset.py#L4) を使用することも可能です。）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p81bdsOerctQ"
      },
      "source": [
        "\"\"\" データセットを分割するためのクラス \"\"\"\n",
        "class Partition(object):\n",
        "\n",
        "    def __init__(self, data, index):\n",
        "        self.data = data\n",
        "        self.index = index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_idx = self.index[index]\n",
        "        return self.data[data_idx]\n",
        "\n",
        "\n",
        "class DataPartitioner(object):\n",
        "\n",
        "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
        "        self.data = data\n",
        "        self.partitions = []\n",
        "        rng = Random()\n",
        "        rng.seed(seed)\n",
        "        data_len = len(data)\n",
        "        indexes = [x for x in range(0, data_len)]\n",
        "        rng.shuffle(indexes)\n",
        "\n",
        "        for frac in sizes:\n",
        "            part_len = int(frac * data_len)\n",
        "            self.partitions.append(indexes[0:part_len])\n",
        "            indexes = indexes[part_len:]\n",
        "\n",
        "    def use(self, partition):\n",
        "        return Partition(self.data, self.partitions[partition])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fw6hgoxrfB3"
      },
      "source": [
        "上記のクラスを利用することで、あとは数行実装すれば、どんなデータセットでもシンプルに分割できるようになりました。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dQ6PpLjrgn9"
      },
      "source": [
        "\"\"\" MNISTの分割 \"\"\"\n",
        "def partition_dataset():\n",
        "    dataset = datasets.MNIST('./data', train=True, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "    size = dist.get_world_size()\n",
        "    bsz = 128 / float(size)\n",
        "    partition_sizes = [1.0 / size for _ in range(size)]\n",
        "    partition = DataPartitioner(dataset, partition_sizes)\n",
        "    partition = partition.use(dist.get_rank())\n",
        "    train_set = torch.utils.data.DataLoader(partition,\n",
        "                                         batch_size=bsz,\n",
        "                                         shuffle=True)\n",
        "    return train_set, bsz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnuL9__6rj1a"
      },
      "source": [
        "2つの複製されたプロセスがあると仮定したとき、各プロセスは 60,000 / 2 = 30,000 個のサンプルを含む `train_set` を有することになります。\n",
        "\n",
        "また、全体としてのバッチサイズを 128 に維持するために、各バッチサイズも複製されたプロセスの数で割ります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQlxvGMfrlvT"
      },
      "source": [
        "これで、通常の（フォワード→バックワード→最適化）の訓練コードを実装し、モデルの勾配を平均化する関数の呼び出し部分を追加することができます。\n",
        "\n",
        "\n",
        "（次のコードの大部分は、公式の [PyTorchのMNISTの例](https://github.com/pytorch/examples/blob/master/mnist/main.py) を基にしています。）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQT8yR_lrnSw"
      },
      "source": [
        "\"\"\" 分散同期SGDの例 \"\"\"\n",
        "def run(rank, size):\n",
        "    torch.manual_seed(1234)\n",
        "    train_set, bsz = partition_dataset()\n",
        "    model = Net()\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=0.01, momentum=0.5)\n",
        "\n",
        "    num_batches = ceil(len(train_set.dataset) / float(bsz))\n",
        "    for epoch in range(10):\n",
        "        epoch_loss = 0.0\n",
        "        for data, target in train_set:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target)\n",
        "            epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            average_gradients(model)\n",
        "            optimizer.step()\n",
        "        print('Rank ', dist.get_rank(), ', epoch ',\n",
        "              epoch, ': ', epoch_loss / num_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzAURi1arpgc"
      },
      "source": [
        "そして、単にモデルを受け取り、ワールド全体の勾配を平均化する `average_gradients(model)` 関数を実装します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpdNnrdurrHk"
      },
      "source": [
        "\"\"\" 勾配の平均化 \"\"\"\n",
        "def average_gradients(model):\n",
        "    size = float(dist.get_world_size())\n",
        "    for param in model.parameters():\n",
        "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)  # all_reduceの働きは、チュートリアル前半の絵をご覧ください\n",
        "        param.grad.data /= size"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy2pRy6Iru9J"
      },
      "source": [
        "いかがでしょうか。\n",
        "\n",
        "分散同期SGDの実装に成功し、大規模なコンピューティングマシン-クラスタ上で任意のモデルを訓練できるようになりました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUwzl4C8rws3"
      },
      "source": [
        "**注意:**\n",
        "\n",
        "後半の実装は、技術的には正しい内容ですが、同期させたSGDを製品化レベルで実装するには、[より沢山のトリック](https://seba-1511.github.io/dist_blog) が必要になります。\n",
        "\n",
        "繰り返しになりますが、製品レベルでは、テストが行われ、最適化されている[DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel) を使用しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFxa8AQjrz62"
      },
      "source": [
        "## 独自のRing-Allreduceの実装\n",
        "さらなるチャレンジとして、DeepSpeech([BaiduのDeepSpeech](https://github.com/baidu-research/baidu-allreduce) )で使用されている、効率的なRing-Allreduceを実装してみましょう。\n",
        "\n",
        "ポイントツーポイント通信（issend / isrecv）を使用することで、とても簡単に実装できます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuEkO0xzr1n9"
      },
      "source": [
        "\"\"\" 手を加えたring-reduceの実装 \"\"\"\n",
        "def allreduce(send, recv):\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    send_buff = send.clone()\n",
        "    recv_buff = send.clone()\n",
        "    accum = send.clone()\n",
        "\n",
        "    left = ((rank - 1) + size) % size\n",
        "    right = (rank + 1) % size\n",
        "\n",
        "    for i in range(size - 1):\n",
        "        if i % 2 == 0:\n",
        "            # send_buff の送信\n",
        "            send_req = dist.isend(send_buff, right)\n",
        "            dist.recv(recv_buff, left)\n",
        "            accum[:] += recv_buff[:]\n",
        "        else:\n",
        "            # recv_buff の送信\n",
        "            send_req = dist.isend(recv_buff, right)\n",
        "            dist.recv(send_buff, left)\n",
        "            accum[:] += send_buff[:]\n",
        "        send_req.wait()\n",
        "    recv[:] = accum[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw3xNPD7r3vI"
      },
      "source": [
        "上記の実装における `allreduce(send, recv)` 関数は、PyTorchのものとは少々異なるインターフェースになっています。\n",
        "\n",
        "具体的には、`recv` テンソルを引数に取り、すべての `send` テンソルの合計を `recv` テンソルに格納しています。\n",
        "\n",
        "<br>\n",
        "\n",
        "ちなみに、上記の実装とDeepSpeech（[BaiduのDeepSpeech](https://github.com/baidu-research/baidu-allreduce) ）の実装ではもう1つの違いがあります。\n",
        "\n",
        "DeepSpeechの実装では、勾配のテンソルを数個の塊に分割し、通信帯域幅を最適に利用できるようにしています。\n",
        "\n",
        "（ヒント:[torch.chunk](https://pytorch.org/docs/stable/torch.html#torch.chunk)）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqy6nGKfr6Uv"
      },
      "source": [
        "## 発展的なトピック\n",
        "\n",
        "これで `torch.distributed` のより発展的な機能を学ぶ準備ができました。\n",
        "\n",
        "網羅する機能が多いため、本セクションは2つのサブセクションに分けます。\n",
        "\n",
        "1. 通信バックエンド: このサブセクションでは、GPU-GPU通信のためのMPIとGlooの使い方を学びます。\n",
        "2. 初期化メソッド: このサブセクションでは、`dist.init_process_group()` の初期調整フェーズのベストな設定方法を学びます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvcxsh6msBg5"
      },
      "source": [
        "### 通信バックエンド\n",
        "\n",
        "`torch.distributed` の最もエレガントな側面の一つは、異なるバックエンド上に抽象化して構築できることです。\n",
        "\n",
        "始めに紹介しましたが、現在PyTorchには3つのバックエンドが実装されています。\n",
        "\n",
        "Gloo、NCCL、そしてMPIです。\n",
        "\n",
        "各バックエンドはユースケースに応じた、異なる仕様を有し、トレードオフの関係にあります。\n",
        "\n",
        "それぞれのバックエンドでサポートされている関数を比較した表は、[こちら](https://pytorch.org/docs/stable/distributed.html#module-torch.distributed)から確認できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO8blYmvsD1F"
      },
      "source": [
        "**Glooバックエンド**\n",
        "\n",
        "これまでのところでは、[Glooバックエンド](https://github.com/facebookincubator/gloo)の拡張的な利用方法をしてきました。\n",
        "\n",
        "Glooバックエンドはコンパイル済みの PyTorch バイナリに含まれており、Linux (0.2 以降) と macOS (1.3 以降) の両方で動作するので、開発プラットフォームとしては非常に便利です。\n",
        "\n",
        "また、CPU上でのすべてのポイントツーポイント、集合演算、及びGPU上でのすべての集合演算をサポートしています。\n",
        "\n",
        "ただし、CUDAのテンソルの集合演算の実装は、NCCLバックエンドが提供するものほど最適化されていません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiUq9sNesFwx"
      },
      "source": [
        "お気づきのように、GPU 上にモデルを配置した場合、分散SGDのさきほどの実装例は動作しません。\n",
        "\n",
        "複数のGPUを使用するには、追加で次のような修正を行います。\n",
        "\n",
        "1. `device = torch.device(\"cuda:{}\".format(rank))` の使用\n",
        "2. `model = Net()` → `model = Net().to(device)`\n",
        "3. `data, target = data.to(device), target.to(device)` の使用\n",
        "\n",
        "上記の修正により、2つのGPU上で訓練を行い、`watch nvidia-smi` によりGPUの使用率を監視することができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GdaahKWsOLD"
      },
      "source": [
        "**MPIバックエンド**\n",
        "\n",
        "メッセージパッシングインターフェース（MPI）は、ハイパフォーマンスコンピューティングの領域で活用されてきた標準的なツールです。\n",
        "\n",
        "MPIバックエンドは、ポイントツーポイント通信と集合通信を可能にするものであり、`torch.distributed` APIの土台でもありました。\n",
        "\n",
        "MPIにはいくつかの実装(例: [Open-MPI](https://www.open-mpi.org/)、 [MVAPICH2](http://mvapich.cse.ohio-state.edu/)、 [Intel MPI](https://software.intel.com/en-us/intel-mpi-library))が存在しており、それぞれが異なる目的に対して最適化されています。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH1Irz3Dap5N"
      },
      "source": [
        "MPIバックエンドを使用する利点は、大規模なコンピュータークラスター上での広い可用性と高水準の最適化にあります。\r\n",
        "\r\n",
        "例えば、[最近の](https://developer.nvidia.com/ibm-spectrum-mpi)　[一部の](https://developer.nvidia.com/mvapich)　[実装](https://www.open-mpi.org/)では、CUDA IPCとGPU Directの技術も利用し、CPUを介したメモリのコピーを行わないようにしています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHhqpSngsQW5"
      },
      "source": [
        "残念ながら、PyTorchのバイナリにMPIの実装は含まれていないため、手作業でリコンパイルする必要があります。\n",
        "\n",
        "ただ幸いなのは、この処理がコンパイル時にPyTorch自身がMPIの実装を探してくれる、というシンプルに完了する点です。\n",
        "\n",
        "次の手順で、PyTorchを [ソースコード](https://github.com/pytorch/pytorch#from-source) からインストールすることで、MPIバックエンドをインストールできます。\n",
        "\n",
        "1. Anacondaの環境を作成、及び起動し、ガイドに倣って前提となるパッケージ等をすべてインストールします。なお、`python setup.py install` の実行はまだ**行わないでください。**\n",
        "2. お好みのMPIの実装を選択し、インストールします。なお、CUDAの使用を想定しているMPIを有効化するには、いくつかの追加手順が必要な場合がある点に注意してください。今回のケースでは、GPUサポートのないOpen-MPIを選びます。`conda install -c conda-forge openmpi`\n",
        "3. 最後に、クローンしたPyTorchのリポジトリで`python setup.py install`を実行します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vgWr-TNsVyO"
      },
      "source": [
        "新規にインストールされたバックエンドのテストを行うには、少し手を加える必要があります。\n",
        "1. `if __name__ == '__main__':` 以下の内容を `init_process(0, 0, run, backend='mpi')` に置き換えます。\n",
        "2. `mpirun -n 4 python myscript.py` を実行します。\n",
        "\n",
        "上記の変更を行う理由は、プロセスを生成する前にMPIが独自の環境を作成する必要があるためです。\n",
        "\n",
        "MPIは独自のプロセスを生成し、次のサブセクション「初期化メソッド」内で解説されているハンドシェイク通信を行うことで、`init_process_group` における `rank` と `size` の引数を不要にします。\n",
        "\n",
        "これは、`mpirun` に追加の引数を渡すことで各プロセスに合わせた計算リソースの調整ができるため、実際には非常に強力です（プロセスごとのコア数、手作業で割り当てる特定ランクへのマシン、[その他](https://www.open-mpi.org/faq/?category=running#mpirun-hostfile)など）。\n",
        "\n",
        "これらを行うことで、他の通信バックエンドと同じように馴染みのある出力が得られるはずです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7tQ0jL0sYmP"
      },
      "source": [
        "**NCCLバックエンド**\n",
        "\n",
        "NCCLバックエンドは、CUDAのテンソルを対象にした集合演算に最適化された実装を提供しています。\n",
        "\n",
        "集合演算にCUDAのテンソルのみを使う場合は、ベストパフォーマンスを得るためにこのバックエンドの使用を検討してください。\n",
        "\n",
        "なお、NCCLバックエンドは、CUDAのサポートを伴っているビルド済みのバイナリに含まれています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tori3o7esY_w"
      },
      "source": [
        "### 初期化メソッド\n",
        "\n",
        "本チュートリアルの最後に、一番最初に呼び出した `dist.init_process_group(backend, init_method)` 関数について説明します。\n",
        "\n",
        "特に、各プロセス間において初期調整のステップを担当する様々な初期化メソッドについて説明します。\n",
        "\n",
        "これらの初期化メソッドは、初期調整をどのように行うかを定義します。\n",
        "\n",
        "ハードウェアのセットアップ状況によりますが、様々な初期化メソッドのいずれかが最適な手法として使用できます。\n",
        "\n",
        "以下のセクションに加えて、[公式のドキュメント](https://pytorch.org/docs/stable/distributed.html#initialization)にも、目を通していただくことをおすすめします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg1V2FW0sazH"
      },
      "source": [
        "**環境変数**\n",
        "\n",
        "本チュートリアルでは、環境変数の初期化メソッドを使用してきました。\n",
        "\n",
        "下記の4つの環境変数をすべてのマシン上で設定することで、すべてのプロセスが適切にマスターに接続できるようになり、他のプロセスの情報を得ることができます。\n",
        "そして、最終的にはそれらのプロセスとハンドシェイク通信を行えるようになります。\n",
        "\n",
        "- `MASTER_PORT`: ランク0のプロセスをホストするマシン上の空きポート\n",
        "- `MASTER_ADDR`: ランク0のプロセスをホストするマシンのIPアドレス\n",
        "- `WORLD_SIZE`: マスターが、待機しているワーカーの数を知るために設定するプロセスの総数\n",
        "- `RANK`: 各プロセスがワーカーのマスターかどうかを判別するために設定する各プロセスのランク"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6WJJD9VsdTi"
      },
      "source": [
        "**共有ファイルシステム**\n",
        "\n",
        "共有ファイルシステムは、すべてのプロセスが共有ファイルシステムにアクセスできる必要があり、共有ファイルを通してプロセスの調整を行います。\n",
        "\n",
        "これは各プロセスがファイルを開き、情報を書き込み、すべてのプロセスが同様の処理を行うまで待機するということを意味します。\n",
        "\n",
        "なお、必要なすべての情報がすべてのプロセスで利用できるようになったあとは、競合を避けるために、ファイルシステムは [fcntl](http://man7.org/linux/man-pages/man2/fcntl.2.html) を通したロック機構を実装しなければなりません。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7c-f4TbshZx"
      },
      "source": [
        "dist.init_process_group(\n",
        "    init_method='file:///mnt/nfs/sharedfile',\n",
        "    rank=args.rank,\n",
        "    world_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFsie_ddsf_J"
      },
      "source": [
        "**TCP**\n",
        "\n",
        "TCP経由での初期化は、ランク0のプロセスのIPアドレスと到達可能なポート番号を提供することで行われます。\n",
        "\n",
        "ここでは、すべてのワーカーがランク0のプロセスに繋がり、相互に到達方法の情報交換ができるようになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxSWFLtgslwK"
      },
      "source": [
        "dist.init_process_group(\n",
        "    init_method='tcp://10.1.1.20:23456',\n",
        "    rank=args.rank,\n",
        "    world_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIkEb7NGskHu"
      },
      "source": [
        "## 謝辞\n",
        "これらの実装、ドキュメント、そしてテストを行ったPyTorchの開発者に感謝いたします。\n",
        "\n",
        "コードへの理解が不足していた際には、[ドキュメント](https://pytorch.org/docs/stable/distributed.html)やテストに頼ることで、答えを見つけることが出来ました。\n",
        "\n",
        "特に、洞察に満ちたコメントや初期の原稿において質問への回答をいただいたSoumith Chintala、Adam Paszke、Natalia Gimelshein には改めて感謝の意を表します。"
      ]
    }
  ]
}