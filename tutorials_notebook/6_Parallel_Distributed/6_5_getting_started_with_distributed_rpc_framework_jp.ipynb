{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "6_5_getting_started_with_distributed_rpc_framework_jp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKHEsZMUt84_"
      },
      "source": [
        "# 「分散RPCフレームワーク入門」\n",
        "\n",
        "【原題】Getting Started with Distributed RPC Framework\n",
        "\n",
        "【原著】[Shen Li](https://mrshenli.github.io/)\n",
        "\n",
        "【元URL】https://pytorch.org/tutorials/intermediate/rpc_tutorial.html\n",
        "\n",
        "【翻訳】電通国際情報サービスISID HCM事業部　櫻井 亮佑\n",
        "\n",
        "【日付】2020年11月28日\n",
        "\n",
        "【チュトーリアル概要】\n",
        "\n",
        "前提知識\n",
        "- [PyTorch Distributedについて](https://pytorch.org/tutorials/beginner/dist_overview.html)（日本語版6_1）\n",
        "- [RPC APIドキュメント](https://pytorch.org/docs/master/rpc.html)\n",
        "\n",
        "<br>\n",
        "\n",
        "本チュートリアルでは、`torch.distributed.rpc` パッケージで分散訓練を構築する方法について、2つのシンプルな例を示しながら解説します。\n",
        "\n",
        "なお、`torch.distributed.rpc` パッケージは、PyTorch v1.4 から初めてプロトタイプ機能として導入されました。\n",
        "\n",
        "2つの例について、ソースコードは、[PyTorchのサンプル例](https://github.com/pytorch/examples)で確認できます。\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyCmnWJpPOeR"
      },
      "source": [
        "以前のチュートリアル（[分散データ並列訓練入門（日本語版6_3）](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)、[PyTorchで実装する分散アプリケーション（日本語版6_4）](https://pytorch.org/tutorials/intermediate/dist_tuto.html)）では、[DistributedDataParallel](https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html) には、複数のプロセスに渡ってモデルを複製して各プロセスが分割された入力データを扱うといった、特定の訓練パラダイムをサポートするものであると解説しました。\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK9mDqp2PZoZ"
      },
      "source": [
        "しかし、時には異なる訓練パラダイムが必要になる場面に直面することがあるかもしれません。\r\n",
        "\r\n",
        "例えば以下のようなケースです。\r\n",
        "\r\n",
        "1. 強化学習においては、モデル自体がかなり小さい一方で、環境から得る必要のある訓練データが比較的大量になることがあり得ます。このケースでは、並列に動作する複数のオブザーバーを生成し、単一のエージェントに共有することが有益であるかもしれません。この際、エージェントはローカルで訓練を行いますが、アプリケーションにはオブザーバーとトレーナーの間でデータを送受信するためのライブラリが必要になります。\r\n",
        "2. 構築するモデルが単一のマシン上のGPUに収めるには大きすぎるかもしれない場合には、複数のマシンにモデルを分割する上で役に立つライブラリが必要になります。\r\n",
        " そうしない場合、モデルのパラメーターとトレーナーが異なるマシン上に存在する状況を対象にした[パラメータサーバー](https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf)の訓練フレームワークを実装する必要があるかもしれません。\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP75o0xrPs0O"
      },
      "source": [
        " [torch.distributed.rpc](https://pytorch.org/docs/master/rpc.html) パッケージは、上記のようなシナリオで役に立ちます。\r\n",
        "\r\n",
        "ケース1では、[RPC](https://pytorch.org/docs/master/rpc.html#rpc)と[RRef](https://pytorch.org/docs/master/rpc.html#rref) を使用することで、リモートのデータオブジェクトを簡単に参照しながら、あるワーカーから別のワーカーへデータを送信することができます。\r\n",
        "\r\n",
        "ケース2では、[分散自動微分](https://pytorch.org/docs/master/rpc.html#distributed-autograd-framework)と[分散オプティマイザー](https://pytorch.org/docs/master/rpc.html#module-torch.distributed.optim)を使うことで、あたかもローカル上での訓練のようにバックワードパスとオプティマイザーステップを実行できます。\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcdBa-MqP0dx"
      },
      "source": [
        "次の2つのセクションでは、強化学習の例と言語モデルの例を用いて、[torch.distributed.rpc](https://pytorch.org/docs/master/rpc.html) のAPIを解説します。\r\n",
        "\r\n",
        "なお、本チュートリアルは、最高の性能や効率のモデルを構築して問題を解くことを目的としておらず、ここでの主な目標は、分散訓練アプリケーションを構築する上で [torch.distributed.rpc](https://pytorch.org/docs/master/rpc.html) を使用する方法を示すことである点に注意してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw4dFG6Zt85J"
      },
      "source": [
        "## RPCとRRefを用いた分散強化学習\n",
        "\n",
        "本セクションでは、[OpenAI Gym](https://gym.openai.com/)のCartPole-v1を対象にRPCを使った、分散強化学習のトイモデル構築手順を解説します。\n",
        "\n",
        "なお、以下に示すように、ポリシーのコードの大部分は、既存のシングルスレッドの[実装例](https://github.com/pytorch/examples/blob/master/reinforcement_learning)から借用しています。\n",
        "\n",
        "`Policy`の設計に関する詳細は省き、RPCの使い方に焦点を当てます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59OhF3VVt85J"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Policy(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.affine1 = nn.Linear(4, 128)\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        self.affine2 = nn.Linear(128, 2)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.affine1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.affine2(x)\n",
        "        return F.softmax(action_scores, dim=1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4WsVzCvt85K"
      },
      "source": [
        "まず、リモートからRRefを保有しているワーカー上で関数を実行する際に役立つ関数を準備します。\n",
        "\n",
        "この関数は、本チュートリアルの中で何回か目にすることになるでしょう。\n",
        "\n",
        "本来であれば、`torch.distributed.rpc`がこのような便利な関数を備えているべきです。\n",
        "\n",
        "例えば、アプリケーションが直接 `RRef.some_func(*arg)` を呼び出すことで、当該関数を、RRefを保有するワーカーに対して処理を行うRPCに変換することができればより楽になります。\n",
        "\n",
        "このAPIに関する進捗は [pytorch/pytorch#31743](https://github.com/pytorch/pytorch/issues/31743) にて管理されています。\n",
        "\n",
        "（日本語訳注：2020年1月4日にAPIはマージされ、2020年6月6日にクローズされています。）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aWKMz2At85K"
      },
      "source": [
        "from torch.distributed.rpc import rpc_sync\n",
        "\n",
        "def _call_method(method, rref, *args, **kwargs):\n",
        "    return method(rref.local_value(), *args, **kwargs)\n",
        "\n",
        "\n",
        "def _remote_method(method, rref, *args, **kwargs):\n",
        "    args = [method, rref] + list(args)\n",
        "    return rpc_sync(rref.owner(), _call_method, args=args, kwargs=kwargs)\n",
        "\n",
        "# rref上で関数を呼び出すには、次の方法で可能です。\n",
        "# _remote_method(some_func, rref, *args)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2hIX3KNt85L"
      },
      "source": [
        "オブザーバーを用意する準備ができました。\n",
        "\n",
        "今回の例では、各オブザーバーが各自で環境を作成し、エピソードを実行させるエージェントからのコマンドを待機します。\n",
        "\n",
        "各エピソードでは、一つのオブザーバーが最大で `n_steps` 回のイテレーションをループし、各イテレーションにおいてRPCを用いて環境の状態をエージェントに伝え、アクションを受け取ります。\n",
        "\n",
        "そして、受け取ったアクションを環境に適用し、環境から報酬と次の状態を得ます。\n",
        "\n",
        "その後、オブザーバーは、RPCを利用して、エージェントに報酬を報告します。\n",
        "\n",
        "なお、繰り返しになりますが、この実装方法は、明らかに効率が最大限に良いオブザーバーの実装方法ではない点に留意してください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_fLmGZfQzMk"
      },
      "source": [
        "例えば、単純な最適化の方法の一つとしては、通信のオーバーヘッドを削減するために現在の状態と最後の報酬を1つのRPCに詰め込むことが考えられます。\r\n",
        "\r\n",
        "しかし、今回の目標はCartPoleを最も上手く攻略することではなく、RPC APIの解説をすることです。\r\n",
        "\r\n",
        "そのため、このチュートリアルではロジックをシンプルにしておき、2つのステップを理解しやすいようにします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5eB-kcHt85L"
      },
      "source": [
        "import argparse\n",
        "import gym\n",
        "import torch.distributed.rpc as rpc\n",
        "\n",
        "parser = argparse.ArgumentParser(\n",
        "    description=\"RPC Reinforcement Learning Example\",\n",
        "    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        ")\n",
        "\n",
        "parser.add_argument('--world_size', default=2, help='Number of workers')\n",
        "parser.add_argument('--log_interval', default=1, help='Log every log_interval episodes')\n",
        "parser.add_argument('--gamma', default=0.1, help='how much to value future rewards')\n",
        "parser.add_argument('--seed', default=1, help='random seed for reproducibility')\n",
        "args = parser.parse_args()\n",
        "\n",
        "class Observer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.id = rpc.get_worker_info().id\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        self.env.seed(args.seed)\n",
        "\n",
        "    def run_episode(self, agent_rref, n_steps):\n",
        "        state, ep_reward = self.env.reset(), 0\n",
        "        for step in range(n_steps):\n",
        "            # 状態をエージェントに送信し、アクションを受け取る。\n",
        "            action = _remote_method(Agent.select_action, agent_rref, self.id, state)\n",
        "\n",
        "            # アクションを環境に適用し、報酬を受け取る。\n",
        "            state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "            # 訓練を行うために、エージェントに報酬を報告する。\n",
        "            _remote_method(Agent.report_reward, agent_rref, self.id, reward)\n",
        "\n",
        "            if done:\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNU5CKO3t85L"
      },
      "source": [
        "エージェントのコードはオブザーバーのコードよりも少し複雑なため、複数のパーツに分けて扱います。\n",
        "\n",
        "今回の例では、エージェントはトレーナーとマスターの両方の役割を果たします。\n",
        "\n",
        "そのため、エージェントは分散した複数のオブザーバーにコマンドを送信してエピソードを実行させるとともに、各エピソードの後の訓練フェーズで使用されるすべてのアクションと報酬をローカルに記録します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yitmu7DsRTRB"
      },
      "source": [
        "以下のコードは、`Agent` コンストラクターとなります。\r\n",
        "\r\n",
        "行のほとんどが、各種コンポーネントの初期化に費やされています。\r\n",
        "\r\n",
        "また、コンストラクターの最後のループでは、他のワーカー上のオブザーバーをリモートで初期化し、ローカルでそれらのオブザーバーへのRRefを保持しています。\r\n",
        "これらのオブザーバーへのRRefsは、後でエージェントがコマンドを送信するために使用します。\r\n",
        "\r\n",
        "なお、アプリケーションは `RRef` の存続について気にする必要はありません。\r\n",
        "\r\n",
        "`RRef` を保有する各ワーカーは、`RRef` の存続を管理するために参照回数マップを維持し、管理対象の `RRef` を保有しているユーザが存在する限り、リモートのデータオブジェクトが削除されないことを保証しています。\r\n",
        "\r\n",
        "詳細は `RRef` の [設計ドキュメント](https://pytorch.org/docs/master/notes/rref.html) を参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn_-DIJ0t85M"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.distributed.rpc as rpc\n",
        "import torch.optim as optim\n",
        "from torch.distributed.rpc import RRef, rpc_async, remote\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, world_size):\n",
        "        self.ob_rrefs = []\n",
        "        self.agent_rref = RRef(self)\n",
        "        self.rewards = {}\n",
        "        self.saved_log_probs = {}\n",
        "        self.policy = Policy()\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n",
        "        self.eps = np.finfo(np.float32).eps.item()\n",
        "        self.running_reward = 0\n",
        "        self.reward_threshold = gym.make('CartPole-v1').spec.reward_threshold\n",
        "        for ob_rank in range(1, world_size):\n",
        "            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n",
        "            self.ob_rrefs.append(remote(ob_info, Observer))\n",
        "            self.rewards[ob_info.id] = []\n",
        "            self.saved_log_probs[ob_info.id] = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6RZGFFnt85M"
      },
      "source": [
        "次に、アクションの選択と報酬の報告を行うために、エージェントは2つのAPIをオブザーバーに向けて公開します。<br>\n",
        "これらの関数は、エージェント上でローカルに実行されますが、RPCを介したオブザーバーによるトリガーによって実行されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU-QXu47t85N"
      },
      "source": [
        "class Agent:\n",
        "    # ...\n",
        "    def select_action(self, ob_id, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs = self.policy(state)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        self.saved_log_probs[ob_id].append(m.log_prob(action))\n",
        "        return action.item()\n",
        "\n",
        "    def report_reward(self, ob_id, reward):\n",
        "        self.rewards[ob_id].append(reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3nx6AHFt85N"
      },
      "source": [
        "エピソードを実行するようにオブザーバーに伝える `run_episode` 関数をエージェントに加えてみましょう。\n",
        "\n",
        "この関数では、まず非同期RPCから future を収集するためのリストを作成し、その後、すべてのオブザーバーのRRefをループして非同期RPCを作成します。\n",
        "\n",
        "なお、これらのRPCでは、オブザーバーがエージェント上の関数を呼び出すことができるのと同様に、エージェントも自身のRRefをオブザーバーに渡します。\n",
        "\n",
        "上に示しましたが、各オブザーバーはエージェントに `RPC` を返しますが、これはネストされた `RPC `です。\n",
        "\n",
        "各エピソードの後、`saved_log_probs` と `rewards` には、記録されたアクションの確率値と報酬が格納されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUq7s4QYt85N"
      },
      "source": [
        "class Agent:\n",
        "    ...\n",
        "    def run_episode(self, n_steps=0):\n",
        "        futs = []\n",
        "        for ob_rref in self.ob_rrefs:\n",
        "            # 非同期RPCを作り、すべてのオブザーバー上のエピソードを開始する。\n",
        "            futs.append(\n",
        "                rpc_async(\n",
        "                    ob_rref.owner(),\n",
        "                    _call_method,\n",
        "                    args=(Observer.run_episode, ob_rref, self.agent_rref, n_steps)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # すべてのオブザーバーがエピソードを終了するまで待機する。\n",
        "        for fut in futs:\n",
        "            fut.wait()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfEDZST9t85O"
      },
      "source": [
        "最後の部分です。\n",
        "1エピソード後、エージェントはモデルを訓練する必要がありますが、この部分については以下の `finish_episode` 関数で実装します。\n",
        "\n",
        "なお、この関数に `RPC` は存在せず、ほとんどがシングルスレッドの例から借用しています。\n",
        "\n",
        "そのため、内容の説明は省略します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7DvATTMt85O"
      },
      "source": [
        "class Agent:\n",
        "    # ...\n",
        "    def finish_episode(self):\n",
        "        # 異なるオブザーバーからの確率値と報酬をリストに結合します。\n",
        "        R, probs, rewards = 0, [], []\n",
        "        for ob_id in self.rewards:\n",
        "            probs.extend(self.saved_log_probs[ob_id])\n",
        "            rewards.extend(self.rewards[ob_id])\n",
        "\n",
        "        # 最小のオブザーバーの報酬を使用して実行報酬を計算します。\n",
        "        min_reward = min([sum(self.rewards[ob_id]) for ob_id in self.rewards])\n",
        "        self.running_reward = 0.05 * min_reward + (1 - 0.05) * self.running_reward\n",
        "\n",
        "        # 保存された確率値と報酬を消去します。\n",
        "        for ob_id in self.rewards:\n",
        "            self.rewards[ob_id] = []\n",
        "            self.saved_log_probs[ob_id] = []\n",
        "\n",
        "        policy_loss, returns = [], []\n",
        "        for r in rewards[::-1]:\n",
        "            R = r + args.gamma * R\n",
        "            returns.insert(0, R)\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + self.eps)\n",
        "        for log_prob, R in zip(probs, returns):\n",
        "            policy_loss.append(-log_prob * R)\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return min_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR7OmPqrt85O"
      },
      "source": [
        "`Policy` クラス、`Observer` クラス、そして `Agent` クラスを用意し、マルチプロセスを立ち上げて分散訓練を行う準備ができました。\n",
        "\n",
        "今回の例では、すべてのプロセスが同じ `run_worker` 関数を実行しますが、各プロセスの役割を区別するためにはランクを使用します。\n",
        "具体的には、ランク0は常にエージェントとし、他のランクはすべてオブザーバーとします。\n",
        "\n",
        "エージェントは、実行中の報酬が環境によって指定された報酬の閾値を超えるまで、`run_episode` と `finish_episode` を繰り返し呼び出すことで、マスターとしての役割を果たします。\n",
        "\n",
        "一方、すべてのオブザーバーは、エージェントからの命令を受動的に待ち続けます。\n",
        "\n",
        "なお、コードは [rpc.init_rpc](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.init_rpc) と [rpc.shutdown](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.shutdown) によってラップされており、それぞれ RPC インスタンスの初期化と終了を行います。\n",
        "\n",
        "詳細は、[APIのページ](https://pytorch.org/docs/master/rpc.html) で確認できます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Y4IpWYt85P"
      },
      "source": [
        "import os\n",
        "from itertools import count\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "AGENT_NAME = \"agent\"\n",
        "OBSERVER_NAME=\"obs\"\n",
        "TOTAL_EPISODE_STEP = 100\n",
        "\n",
        "def run_worker(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    if rank == 0:\n",
        "        # rank0はエージェントです。\n",
        "        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n",
        "\n",
        "        agent = Agent(world_size)\n",
        "        for i_episode in count(1):\n",
        "            n_steps = int(TOTAL_EPISODE_STEP / (args.world_size - 1))\n",
        "            agent.run_episode(n_steps=n_steps)\n",
        "            last_reward = agent.finish_episode()\n",
        "\n",
        "            if i_episode % args.log_interval == 0:\n",
        "                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                      i_episode, last_reward, agent.running_reward))\n",
        "\n",
        "            if agent.running_reward > agent.reward_threshold:\n",
        "                print(\"Solved! Running reward is now {}!\".format(agent.running_reward))\n",
        "                break\n",
        "    else:\n",
        "        # それ以外のランクはオブザーバーです。\n",
        "        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n",
        "        # オブザーバーは、エージェントからの指令を受動的に待ち続けます。\n",
        "\n",
        "    # すべてのRPCが終了するまでブロックし、その後RPCインスタンスをシャットダウンします。\n",
        "    rpc.shutdown()\n",
        "\n",
        "\n",
        "mp.spawn(\n",
        "    run_worker,\n",
        "    args=(args.world_size, ),\n",
        "    nprocs=args.world_size,\n",
        "    join=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbLdjxMBt85Q"
      },
      "source": [
        "world_size=2 で訓練した際の出力例の一部は以下の通りです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THec-FEd_YnE"
      },
      "source": [
        "```\r\n",
        "Episode 10      Last reward: 26.00      Average reward: 10.01\r\n",
        "Episode 20      Last reward: 16.00      Average reward: 11.27\r\n",
        "Episode 30      Last reward: 49.00      Average reward: 18.62\r\n",
        "Episode 40      Last reward: 45.00      Average reward: 26.09\r\n",
        "Episode 50      Last reward: 44.00      Average reward: 30.03\r\n",
        "Episode 60      Last reward: 111.00     Average reward: 42.23\r\n",
        "Episode 70      Last reward: 131.00     Average reward: 70.11\r\n",
        "Episode 80      Last reward: 87.00      Average reward: 76.51\r\n",
        "Episode 90      Last reward: 86.00      Average reward: 95.93\r\n",
        "Episode 100     Last reward: 13.00      Average reward: 123.93\r\n",
        "Episode 110     Last reward: 33.00      Average reward: 91.39\r\n",
        "Episode 120     Last reward: 73.00      Average reward: 76.38\r\n",
        "Episode 130     Last reward: 137.00     Average reward: 88.08\r\n",
        "Episode 140     Last reward: 89.00      Average reward: 104.96\r\n",
        "Episode 150     Last reward: 97.00      Average reward: 98.74\r\n",
        "Episode 160     Last reward: 150.00     Average reward: 100.87\r\n",
        "Episode 170     Last reward: 126.00     Average reward: 104.38\r\n",
        "Episode 180     Last reward: 500.00     Average reward: 213.74\r\n",
        "Episode 190     Last reward: 322.00     Average reward: 300.22\r\n",
        "Episode 200     Last reward: 165.00     Average reward: 272.71\r\n",
        "Episode 210     Last reward: 168.00     Average reward: 233.11\r\n",
        "Episode 220     Last reward: 184.00     Average reward: 195.02\r\n",
        "Episode 230     Last reward: 284.00     Average reward: 208.32\r\n",
        "Episode 240     Last reward: 395.00     Average reward: 247.37\r\n",
        "Episode 250     Last reward: 500.00     Average reward: 335.42\r\n",
        "Episode 260     Last reward: 500.00     Average reward: 386.30\r\n",
        "Episode 270     Last reward: 500.00     Average reward: 405.29\r\n",
        "Episode 280     Last reward: 500.00     Average reward: 443.29\r\n",
        "Episode 290     Last reward: 500.00     Average reward: 464.65\r\n",
        "Solved! Running reward is now 475.3163778435275!\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLZM7Pqmt85R"
      },
      "source": [
        "今回の例では、ワーカー間でデータを渡すための通信伝達手段として RPC を使用する方法と、リモートのオブジェクトを参照するために RRef を使用する方法を紹介しました。\n",
        "\n",
        "その他には、`ProcessGroup` の `send` APIと `recv` API上に構造全体を直接構築したり、他の通信/RPCライブラリを使用することも可能です。\n",
        "\n",
        "しかし、`torch.distributed.rpc` を使用することで、ネイティブサポートと継続的に最適化されたパフォーマンスを自動的に活用できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0JxRotZt85R"
      },
      "source": [
        "次のセクションでは、RPCとRRefを分散自動微分と分散オプティマイザーに組み込み、分散モデル並列訓練を実施する方法を解説します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pP51K5Nt85R"
      },
      "source": [
        "## 分散自動微分と分散オプティマイザーを用いた分散RNN\n",
        "\n",
        "本セクションでは、RNNモデルを用いて、分散モデル並列訓練をRPC APIで行う方法について説明します。\n",
        "\n",
        "なお、今回の例で使用するRNNモデルはとても小さく、単一のGPUにも容易に収まりますが、考え方を解説する目的で2つの異なるワーカーに層を分割します。\n",
        "\n",
        "また、同様のテクニックを適用することで、開発者は複数のデバイスやマシンに対して、大規模なモデルを分散させることができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65UrVgA_t85R"
      },
      "source": [
        "RNNモデルの設計は、PyTorchの [サンプル例](https://github.com/pytorch/examples/tree/master/word_language_model) のリポジトリにある、単語の言語モデルから借用します。\n",
        "\n",
        "\n",
        "このモデルには、埋め込みテーブル、LSTM層、デコーダーの3つの主要なコンポーネントが含まれています。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKG8yDNZSoUQ"
      },
      "source": [
        "以下のコードは、埋め込みテーブルとデコーダーを1つのサブモジュールにラップし、それらのコンストラクターをRPC APIに渡すようにしています。\r\n",
        "\r\n",
        "なお、`EmbeddingTable` サブモジュールでは、ユースケースを網羅するために、意図的に `Embedding` 層をGPU上に配置しています。\r\n",
        "\r\n",
        "v1.4では、RPCは常にCPUのテンソルの引数や戻り値を宛先のワーカー上に作成します。\r\n",
        "\r\n",
        "そのため、関数がGPU上のテンソルを扱う場合、明示的に適切なデバイスにテンソルを移動させる必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMSv8BPHt85R"
      },
      "source": [
        "class EmbeddingTable(nn.Module):\n",
        "    \"\"\"\n",
        "    RNNModelのエンコード層\n",
        "    \"\"\"\n",
        "    def __init__(self, ntoken, ninp, dropout):\n",
        "        super(EmbeddingTable, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp).cuda()\n",
        "        self.encoder.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.drop(self.encoder(input.cuda()).cpu()\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, ntoken, nhid, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def forward(self, output):\n",
        "        return self.decoder(self.drop(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RWmHjF7t85S"
      },
      "source": [
        "上記の2つのサブモジュールにより、RPCを使ってRNNモデルを作成することができるようになりました。\n",
        "\n",
        "以下の実装コードでは、`ps` が埋め込みテーブルとデコーダーのパラメーターを保有するパラメーターサーバーを表しています。\n",
        "\n",
        "コンストラクターはリモートAPIを使用し、パラメーターサーバー上で `EmbeddingTable` オブジェクトと `Decoder` オブジェクトを作成した後、`LSTM` サブモジュールをローカルに作成します。\n",
        "\n",
        "フォワードパスの間に、トレーナーは `EmbeddingTable` のRRefを使ってリモートのサブモジュールを見つけ、RPCで入力データを `EmbeddingTable` に渡し、その後、照会結果を受け取ります。\n",
        "\n",
        "次に、ローカルの `LSTM` 層を介して埋め込みを実行し、最後に別のRPCを使用して `Decoder` サブモジュールに出力を送信します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQSDwEBbTNw4"
      },
      "source": [
        "なお一般的に、分散モデル並列訓練を実装する際に開発者は、モデルをサブモジュールに分割し、RPCを呼び出してリモートにサブモジュールインスタンスを作成することで、必要に応じてRRefを使用してそれらのサブモジュールインスタンスを見つけることが可能です。\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "下記の実装コードを見てわかるように、シングルマシンモデル並列訓練にとても似たコードになっています。\r\n",
        "\r\n",
        "シングルマシンモデル並列訓練との主な違いは、`Tensor.to(device)` をRPCの関数に置き換えている点です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSuxndwpt85S"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, ps, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "\n",
        "        # リモートで埋め込みテーブルのセットアップ\n",
        "        self.emb_table_rref = rpc.remote(ps, EmbeddingTable, args=(ntoken, ninp, dropout))\n",
        "        # ローカルでLSTMのセットアップ\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        # リモートでデコーダーのセットアップ\n",
        "        self.decoder_rref = rpc.remote(ps, Decoder, args=(ntoken, nhid, dropout))\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # 入力をリモートの埋め込みテーブルに渡し、埋め込みテンソルを受け取る\n",
        "        emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # 出力をリモートのデコーダーに渡し、デコードされた出力を受け取る\n",
        "        decoded = _remote_method(Decoder.forward, self.decoder_rref, output)\n",
        "        return decoded, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYB10BhNt85S"
      },
      "source": [
        "分散オプティマイザーについて紹介をする前に、モデルのパラメーターのRRefの配列を生成する上で役に立つ関数を加え、分散オプティマイザーがその配列を扱えるようにしておきましょう。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlbETHqdTe6w"
      },
      "source": [
        "ローカルでの訓練では、アプリケーションは `Module.parameters()` を呼び出すことですべてのパラメーターのテンソルを照会でき、パラメーター更新のために、それらのテンソルをローカルのオプティマイザーに渡していました。\r\n",
        "\r\n",
        "しかし、一部のパラメーターがリモートのマシン上に存続するため、同じAPIでは分散訓練のケースでは機能しません。\r\n",
        "\r\n",
        "そのため、パラメーターのテンソルの配列を扱う代わりに、分散オプティマイザーはRRefの配列を扱います。\r\n",
        "\r\n",
        "なお、ローカルとリモート双方のモデルのパラメーターについて、モデルのパラメーター毎に一つのRRefを用意します。\r\n",
        "\r\n",
        "作成する関数はいたって単純です。\r\n",
        "\r\n",
        "`Module.parameters()` を呼び出し、パラメーターごとにローカルのRRefを作成するだけです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTMCSXJRt85S"
      },
      "source": [
        "def _parameter_rrefs(module):\n",
        "    param_rrefs = []\n",
        "    for param in module.parameters():\n",
        "        param_rrefs.append(RRef(param))\n",
        "    return param_rrefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWycZfgwt85S"
      },
      "source": [
        "そして、`RNNModel` は3つのサブモジュールを含んでいるため、`_parameter_rrefs` を3回呼び出す必要があります。\n",
        "この処理を別の関数にラップします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMi65Tjyt85T"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    # ...\n",
        "    def parameter_rrefs(self):\n",
        "        remote_params = []\n",
        "        # 埋め込みテーブルのRRefを取得\n",
        "        remote_params.extend(_remote_method(_parameter_rrefs, self.emb_table_rref))\n",
        "        # ローカルのパラメーターのRRefを作成\n",
        "        remote_params.extend(_parameter_rrefs(self.rnn))\n",
        "        # デコーダーのRRefを取得\n",
        "        remote_params.extend(_remote_method(_parameter_rrefs, self.decoder_rref))\n",
        "        return remote_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBF_AsW-t85T"
      },
      "source": [
        "以上で、訓練ループを実装する準備ができました。\n",
        "\n",
        "モデルの引数を初期化した後に、`RNNModel` と `DistributedOptimizer` を作成します。\n",
        "\n",
        "分散オプティマイザーはパラメーターのRRefの配列を引数に取り、RRefを保有しているすべてのワーカーを見つけます。\n",
        "\n",
        "そして、与えられた引数（例：`lr=0.05`）を用いて、RRefを保有している各ワーカーにローカルオプティマイザー を作成します(今回のケースでは `SGD` ですが、他のオプティマイザーも使用可能)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUcRNBqUt85T"
      },
      "source": [
        "訓練ループでは、まず分散自動微分のコンテクストを作成し、分散自動微分エンジンが勾配、そして関連するRPCである `send` / `recv`関数を見つけられるようにします。\n",
        "\n",
        "なお、分散自動微分エンジンの詳細な設計については、[設計について](https://pytorch.org/docs/master/notes/distributed_autograd.html)で確認できます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhglDEshT8Qs"
      },
      "source": [
        "そして、ローカルモデルのようにフォワードパスに取り掛かり、分散バックワードパスを実行します。\r\n",
        "\r\n",
        "分散バックワードでは、対象となる配列を指定するだけです。今回の場合は、損失の `Tensor` です。\r\n",
        "\r\n",
        "分散自動微分エンジンは、分散された計算グラフを自動的に横断し、各ノードに勾配を適切に書き込みます。\r\n",
        "\r\n",
        "次に、分散オプティマイザー上で `step` 関数を実行することで、すべての関連するローカルオプティマイザーに到達し、モデルのパラメーターを更新します。\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV3h2psfUEey"
      },
      "source": [
        "なお、ローカルでの訓練と比較して異なる細かい点としては、 `zero_grad()` を実行する必要がない点です。\r\n",
        "\r\n",
        "これは、自動微分の各コンテクストが、勾配を格納するための専用の領域を有していますが、このコンテクストはイテレーションごとに作り直されるため、異なるイテレーションからの勾配が同じ `Tensors` のセットに蓄積されることがないためです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaWD_Hxyt85T"
      },
      "source": [
        "def run_trainer():\n",
        "    batch = 5\n",
        "    ntoken = 10\n",
        "    ninp = 2\n",
        "\n",
        "    nhid = 3\n",
        "    nindices = 3\n",
        "    nlayers = 4\n",
        "    hidden = (\n",
        "        torch.randn(nlayers, nindices, nhid),\n",
        "        torch.randn(nlayers, nindices, nhid)\n",
        "    )\n",
        "\n",
        "    model = rnn.RNNModel('ps', ntoken, ninp, nhid, nlayers)\n",
        "\n",
        "    # 分散オプティマイザーのセットアップ\n",
        "    opt = DistributedOptimizer(\n",
        "        optim.SGD,\n",
        "        model.parameter_rrefs(),\n",
        "        lr=0.05,\n",
        "    )\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def get_next_batch():\n",
        "        for _ in range(5):\n",
        "            data = torch.LongTensor(batch, nindices) % ntoken\n",
        "            target = torch.LongTensor(batch, ntoken) % nindices\n",
        "            yield data, target\n",
        "\n",
        "    # 10イテレーションの訓練\n",
        "    for epoch in range(10):\n",
        "        for data, target in get_next_batch():\n",
        "            # 分散自動微分のコンテクストを作成\n",
        "            with dist_autograd.context() as context_id:\n",
        "                hidden[0].detach_()\n",
        "                hidden[1].detach_()\n",
        "                output, hidden = model(data, hidden)\n",
        "                loss = criterion(output, target)\n",
        "                # 分散バックワードパスの実行\n",
        "                dist_autograd.backward(context_id, [loss])\n",
        "                # 分散オプティマイザーの実行\n",
        "                opt.step(context_id)\n",
        "                # 勾配は、毎イテレーションでリセットされる分散自動微分に\n",
        "                # 累積されていくため、\n",
        "                # 勾配をゼロ化する必要はありません。\n",
        "        print(\"Training epoch {}\".format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2foUz4N2t85T"
      },
      "source": [
        "最後に、パラメータサーバーと訓練プロセスを起動するためのグルーコード（2つのモジュールを接着する補助関数）を追加しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_B4oWrHt85U"
      },
      "source": [
        "def run_worker(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    if rank == 1:\n",
        "        rpc.init_rpc(\"trainer\", rank=rank, world_size=world_size)\n",
        "        _run_trainer()\n",
        "    else:\n",
        "        rpc.init_rpc(\"ps\", rank=rank, world_size=world_size)\n",
        "        # パラメーターサーバーは特に何も行いません。\n",
        "        pass\n",
        "\n",
        "    # すべてのrpcが終了するまで、処理をブロックします。\n",
        "    rpc.shutdown()\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    world_size = 2\n",
        "    mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}