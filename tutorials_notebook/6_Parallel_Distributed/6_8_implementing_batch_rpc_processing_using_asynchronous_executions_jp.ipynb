{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "6_8_implementing_batch_rpc_processing_using_asynchronous_executions_jp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79FD81P5-tA8"
      },
      "source": [
        "# 「非同期実行を用いたバッチRPC処理の実装」\n",
        "\n",
        "【原題】Implementing Batch RPC Processing Using Asynchronous Executions\n",
        "\n",
        "【原著】[Shen Li](https://mrshenli.github.io/)\n",
        "\n",
        "【元URL】https://pytorch.org/tutorials/intermediate/rpc_async_execution.html\n",
        "\n",
        "【翻訳】電通国際情報サービスISID HCM事業部　櫻井 亮佑\n",
        "\n",
        "【日付】2020年12月05日\n",
        "\n",
        "【チュトーリアル概要】\n",
        "\n",
        "前提知識:\n",
        "- [PyTorch Distributedの概要](https://pytorch.org/tutorials/beginner/dist_overview.html)（日本語版6_1）\n",
        "- [分散RPCフレームワーク入門](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)（日本語版6_5）\n",
        "- [分散RPCフレームワークを用いたパラメーターサーバーの実装](https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html)（日本語版6_6）\n",
        "- [RPCの非同期実行デコレーター](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution)\n",
        "\n",
        "<br>\n",
        "\n",
        "本チュートリアルでは、ブロックされているRPCスレッドの数を減らしつつ、呼び出し先でCUDAの操作を統合することで訓練の高速化を補助する [@rpc.functions.async_execution](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution) を使用して、バッチ処理を行うRPCアプリケーションを構築する方法を解説します。\n",
        "\n",
        "なお、この方法は、[TorchServe\n",
        "でのバッチ推論](https://pytorch.org/serve/batch_inference_with_ts.html) と同じ流れで構築されています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxSNuQFb-tBB"
      },
      "source": [
        "**注意**<br>\n",
        "本チュートリアルにはPyTorch v1.6.0以上が必要です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbjdimuP-tBB"
      },
      "source": [
        "## 基本的な内容\n",
        "\n",
        "前のチュートリアル（日本語版6_7）では、[torch.distributed.rpc](https://pytorch.org/docs/stable/rpc.html) を使って分散訓練アプリケーションを構築する手順を示しましたが、RPCのリクエストを処理する際の呼び出し先については深く追求しませんでした。\n",
        "\n",
        "\n",
        "PyTorch v1.5において、各RPCは、リクエスト内の関数が値を返すまで、呼び出し先で一つのスレッドをブロックした状態で関数を実行します。\n",
        "\n",
        "この仕組みは多くのケースで通用しますが、注意点が一つあります。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNPuJ4h-jpWB"
      },
      "source": [
        "IO上でユーザー関数がブロックされている場合、例えば、ネストされたRPCの実施や信号の送信をしているケース、あるいは異なるRPCのリクエストのブロックの解除を待機しているケースでは、呼び出し先のRPCスレッドは、IOが終了するか信号を送信するイベントが発生するまでは、アイドル状態で待機する必要があります。\r\n",
        "\r\n",
        "結果として、RPCの呼び出し先は必要以上に多くのスレッドを使用するようになります。\r\n",
        "\r\n",
        "この問題の原因は、RPCがユーザー関数をブラックボックスに取り扱っており、関数内で行われる処理をほとんど認識していない点にあります。\r\n",
        "\r\n",
        "ユーザー関数がRPCスレッドを生成、解放できるようにするには、RPCシステムにより多くのヒントとなる情報を提供してあげる必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTioHvNw-tBC"
      },
      "source": [
        "v1.6.0より、PyTorchでは2つの新しい概念を導入し、この問題に対処しています。\n",
        "- 非同期実行をカプセル化し、コールバック関数のインストールもサポートしている [torch.futures.Future](https://pytorch.org/docs/master/futures.html)型\n",
        "- 対象の関数でfutureを返し、実行中に一時停止と複数回の生成ができることを、アプリケーションが呼び出し先に伝えるための [rpc.functions.async_execution](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution)デコレーター"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guTbHBt6-tBC"
      },
      "source": [
        "上記の2つのツールにより、アプリケーションコードはユーザー関数を複数のより小さな関数に細分化することが可能になり、`Future`オブジェクト上でコールバックとしてそれらの関数をつなぎ合わせ、最終的な結果を含む `Future` を返せるようになります。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGz46YbAj--u"
      },
      "source": [
        "呼び出し先では、`Future` オブジェクトを取得する際、後続のRPCのレスポンスの準備とコールバックとしての通信をインストールしますが、これらの処理は最終的な結果の準備ができたときにトリガーされます。\r\n",
        "\r\n",
        "このようにすることで、呼び出し先はスレッドをブロックする必要がなくなり、最終的な返り値の準備が出来るまで待機する必要もなくなります。\r\n",
        "\r\n",
        "単純なサンプル例については、[@rpc.functions.async_execution](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution) のAPIドキュメントを参照ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWSyYO3I-tBC"
      },
      "source": [
        "呼び出し先でのアイドル状態のスレッド数を減らす他に、これらのツールはバッチRPC処理のさらなる簡易化、高速化に貢献します。\n",
        "\n",
        "\n",
        "本チュートリアル内で紹介する2つのセクションでは、[@rpc.functions.async_execution](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution)デコレーターを使用して、分散バッチ更新パラメーターサーバーと、バッチ処理強化学習アプリケーションを構築する方法を解説します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXrnl7Tc-tBC"
      },
      "source": [
        "## バッチ更新パラメーターサーバー\n",
        "\n",
        "一つのパラメーターサーバー（PS）と複数のトレーナーを備えた、同期型パラメーターサーバー訓練アプリケーションについて解説します。\n",
        "\n",
        "このアプリケーションでは、PSがパラメーターを保持し、すべてのトレーナーが報告する勾配を待機します。\n",
        "\n",
        "また、毎イテレーションにおいて、すべてのトレーナーから勾配を受け取るまで待機し、勾配を受け取った後ですべてのパラメーターを一度に更新します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL5A1-zDkW2q"
      },
      "source": [
        "下記のコードは、PSクラスの実装を示しています。\r\n",
        "\r\n",
        "`update_and_fetch_model` メソッドは、`@rpc.functions.async_execution` で修飾されており、トレーナーによって呼び出されるメソッドです。\r\n",
        "\r\n",
        "そして、`update_and_fetch_model` メソッドは呼び出しごとに、更新されたモデルが格納される `Future` オブジェクトを返します。\r\n",
        "\r\n",
        "ほとんどのトレーナーから起動される呼び出しは、`.grad` フィールドに勾配を蓄積して即時に値を返し、PS上でRPCスレッドを生成するだけです。\r\n",
        "\r\n",
        "そして、最後に到着するトレーナーは、オプティマイザーステップをトリガーし、それまでに報告された勾配をすべて処理します。\r\n",
        "\r\n",
        "処理後、更新されたモデルを `future_model` に設定し、`Future` オブジェクトを通じて他のトレーナーからの以前のリクエストを通知して、すべてのトレーナーに更新されたモデルを送信します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5CY9BWy-tBD"
      },
      "source": [
        "import threading\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.distributed.rpc as rpc\n",
        "from torch import optim\n",
        "\n",
        "num_classes, batch_update_size = 30, 5\n",
        "\n",
        "class BatchUpdateParameterServer(object):\n",
        "    def __init__(self, batch_update_size=batch_update_size):\n",
        "        self.model = torchvision.models.resnet50(num_classes=num_classes)\n",
        "        self.lock = threading.Lock()\n",
        "        self.future_model = torch.futures.Future()\n",
        "        self.batch_update_size = batch_update_size\n",
        "        self.curr_update_size = 0\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
        "        for p in self.model.parameters():\n",
        "            p.grad = torch.zeros_like(p)\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n",
        "    @staticmethod\n",
        "    @rpc.functions.async_execution\n",
        "    def update_and_fetch_model(ps_rref, grads):\n",
        "        # RRefを使用してローカルのPSインスタンスを取得\n",
        "        self = ps_rref.local_value()\n",
        "        with self.lock:\n",
        "            self.curr_update_size += 1\n",
        "            # 勾配を累積して .grad フィールドへ\n",
        "            for p, g in zip(self.model.parameters(), grads):\n",
        "                p.grad += g\n",
        "            \n",
        "            # 現在の future_model を保存し、返り値として返し、\n",
        "            # このスレッドが値を返す前に別のスレッドが future_model に手を加えたとしても\n",
        "            # 返す Future オブジェクトが正しいモデルを保持するようにする。\n",
        "            fut = self.future_model\n",
        "\n",
        "            if self.curr_update_size >= self.batch_update_size:\n",
        "                # モデルを更新\n",
        "                for p in self.model.parameters():\n",
        "                    p.grad /= self.batch_update_size\n",
        "                self.curr_update_size = 0\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "                # 結果を Future オブジェクト上に設定し、\n",
        "                # この更新されたモデルを求めているすべての過去のリクエストに通知され、\n",
        "                # それらのリクエストに応じてレスポンスが送信されます。\n",
        "                fut.set_result(self.model)\n",
        "                self.future_model = torch.futures.Future()\n",
        "\n",
        "        return fut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1i4fWCh-tBE"
      },
      "source": [
        "トレーナーについては、PSから得られる同じパラメーターのセットを使用してすべて初期化されます。\n",
        "\n",
        "そして、各イテレーションにおいてそれぞれのトレーナーは、最初にフォワードパスとバックワードパスを実行し、ローカルに勾配を生成します。\n",
        "\n",
        "その後、各トレーナーはRPCを用いて勾配をPSに報告し、同一のRPCリクエストの返り値を通して更新されたパラメーターを受け取ります。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSFBobik0oR"
      },
      "source": [
        "なお、トレーナーの実装において、目的の関数が `@rpc.functions.async_execution` で修飾されているか否かで違いは生じません。\r\n",
        "\r\n",
        "トレーナーはシンプルであり、更新されたモデルが返ってくるまでトレーナー上で処理をブロックする `rpc_sync` を使用して、`update_and_fetch_model` を呼び出すだけとなります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xb-uBjx-tBE"
      },
      "source": [
        "batch_size, image_w, image_h  = 20, 64, 64\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, ps_rref):\n",
        "        self.ps_rref, self.loss_fn = ps_rref, torch.nn.MSELoss()\n",
        "        self.one_hot_indices = torch.LongTensor(batch_size) \\\n",
        "                                    .random_(0, num_classes) \\\n",
        "                                    .view(batch_size, 1)\n",
        "\n",
        "    def get_next_batch(self):\n",
        "        for _ in range(6):\n",
        "            inputs = torch.randn(batch_size, 3, image_w, image_h)\n",
        "            labels = torch.zeros(batch_size, num_classes) \\\n",
        "                        .scatter_(1, self.one_hot_indices, 1)\n",
        "            yield inputs.cuda(), labels.cuda()\n",
        "\n",
        "    def train(self):\n",
        "        name = rpc.get_worker_info().name\n",
        "        # モデルパラメーターの初期値を取得\n",
        "        m = self.ps_rref.rpc_sync().get_model().cuda()\n",
        "        # 訓練開始\n",
        "        for inputs, labels in self.get_next_batch():\n",
        "            self.loss_fn(m(inputs), labels).backward()\n",
        "            m = rpc.rpc_sync(\n",
        "                self.ps_rref.owner(),\n",
        "                BatchUpdateParameterServer.update_and_fetch_model,\n",
        "                args=(self.ps_rref, [p.grad for p in m.cpu().parameters()]),\n",
        "            ).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KCzu_4A-tBE"
      },
      "source": [
        "本チュートリアルでは、マルチプロセスを起動するコードを省略しており、実装の全内容については、[サンプルコード](https://github.com/pytorch/examples/tree/master/distributed/rpc) のリポジトリを参照してください。\n",
        "\n",
        "\n",
        "なお、[@rpc.functions.async_execution](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution)デコレーターが無くても、バッチ処理の実装が可能である点には留意してください。\n",
        "\n",
        "しかし、[@rpc.functions.async_execution](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution)デコレーター無しで実装する場合、PS上でより多くのRPCスレッドをブロックするか、更新されたモデルを受け取る別のRPCを使用する必要が生じます。\n",
        "\n",
        "さらに後者の場合においては、よりコードが複雑になり、通信のオーバーヘッドも増加します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GsCyvv0-tBF"
      },
      "source": [
        "本セクションでは、単純なパラメーターサーバー訓練の例を扱い、[@rpc.functions.async_execution](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution)デコレーターを用いてバッチRPCアプリケーションを実装する方法を示しました。\n",
        "\n",
        "次のセクションでは、以前のチュートリアル [分散RPCフレームワーク入門](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html) （日本語版6_5）で扱った強化学習の例を、バッチ処理を用いて再実装し、バッチ処理が訓練スピードに与える影響について解説します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baP_GFW--tBF"
      },
      "source": [
        "## バッチ処理カートポールソルバー\n",
        "\n",
        "本セクションでは、[OpenAI Gym](https://gym.openai.com/) よりCartPole-v1を例として使用し、バッチ処理RPCのパフォーマンス面での影響をお見せします。\n",
        "\n",
        "なお、最良のカートポールソルバーを構築することや最難関のRLの課題を解くことが目標ではなく、[@rpc.functions.async_execution](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution) の使用方法を解説することが目標である点に留意してください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "202WJz-WmFki"
      },
      "source": [
        "下記に示すように、以前のチュートリアルで使用したものと同様の `Policy` モデルを使います。\r\n",
        "\r\n",
        "以前のチュートリアルと比較した際の違いは、コンストラクターにおいて、`F.softmax` の `dim` パラメーターを制御するために、 引数`batch` を追加している点です。\r\n",
        "\r\n",
        "\r\n",
        "これは、バッチ処理を行う際、`forward` 関数内の引数 `x` が複数のオブザーバーから得た状態を含んでいるため、次元が適切に変更される必要があるためです。\r\n",
        "\r\n",
        "他の部分はすべてそのままです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfiwCSuc-tBF"
      },
      "source": [
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch RPC Batch RL example')\n",
        "parser.add_argument('--gamma', type=float, default=1.0, metavar='G',\n",
        "                    help='discount factor (default: 1.0)')\n",
        "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
        "                    help='random seed (default: 543)')\n",
        "parser.add_argument('--num-episode', type=int, default=10, metavar='E',\n",
        "                    help='number of episodes (default: 10)')\n",
        "args = parser.parse_args()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, batch=True):\n",
        "        super(Policy, self).__init__()\n",
        "        self.affine1 = nn.Linear(4, 128)\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        self.affine2 = nn.Linear(128, 2)\n",
        "        self.dim = 2 if batch else 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.affine1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.affine2(x)\n",
        "        return F.softmax(action_scores, dim=self.dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeN9TXEk-tBG"
      },
      "source": [
        "`Observer` のコンストラクターも同様に修正を行います。\n",
        "\n",
        "すなわち、引数`batch`を取り、アクションを選択するために使用する `Agent` の関数を決定します。\n",
        "\n",
        "そしてObserberは、バッチモードの場合、この後実装する `Agent` 上で `select_action_batch` を呼び出しますが、この関数が [@rpc.functions.async_execution](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution) で修飾されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJp7Ylt6-tBG"
      },
      "source": [
        "import gym\n",
        "import torch.distributed.rpc as rpc\n",
        "\n",
        "class Observer:\n",
        "    def __init__(self, batch=True):\n",
        "        self.id = rpc.get_worker_info().id - 1\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        self.env.seed(args.seed)\n",
        "        self.select_action = Agent.select_action_batch if batch else Agent.select_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73nUPCIF-tBG"
      },
      "source": [
        "前のチュートリアル [分散RPCフレームワーク入門](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html) （日本語版6_5）と比較すると、オブザーバーは少し異なった振る舞いをします。\n",
        "\n",
        "オブザーバーは、環境が止まった際に離脱する代わりに、各エピソード毎で常に `n_steps` 回イテレーションを実行します。\n",
        "\n",
        "そして、環境が返ってくる際に、単にオブザーバーは環境をリセットし、また最初からやり直します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiCMaLxRnDMd"
      },
      "source": [
        "この設計では、エージェントは各オブザーバーから一定数の状態を受け取るため、それらの状態を固定長のテンソルへと詰め込むことができます。\r\n",
        "\r\n",
        "各ステップでは、`Observer` がRPCを使用して `Agent` に状態を送信し、そして返り値を通してアクションを受け取ります。\r\n",
        "\r\n",
        "そして、各エピソードの終了時には、すべてのステップの報酬を `Agent` に返します。\r\n",
        "\r\n",
        "なお、この `run_episode` 関数は、RPCを用いて `Agent` によって呼び出される点に留意してください。\r\n",
        "\r\n",
        "そのため、この関数内での `rpc_sync` の呼び出しは、ネストされたRPCの使用になります。\r\n",
        "\r\n",
        "ちなみに、この関数を `@rpc.functions.async_execution` でマークすることで、`Observer`上で発生するスレッドのブロックを避けることも可能です。\r\n",
        "\r\n",
        "しかし、`Observer` ではなく `Agent` がボトルネックであるため、`Observer` のプロセス上のスレッドをブロックしてしまうことは、問題ないはずです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FQJsQor-tBH"
      },
      "source": [
        "import torch\n",
        "\n",
        "class Observer:\n",
        "    # ...\n",
        "\n",
        "    def run_episode(self, agent_rref, n_steps):\n",
        "        state, ep_reward = self.env.reset(), NUM_STEPS\n",
        "        rewards = torch.zeros(n_steps)\n",
        "        start_step = 0\n",
        "        for step in range(n_steps):\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "            # 状態をエージェントに送信し、アクションを取得\n",
        "            action = rpc.rpc_sync(\n",
        "                agent_rref.owner(),\n",
        "                self.select_action,\n",
        "                args=(agent_rref, self.id, state)\n",
        "            )\n",
        "\n",
        "            # アクションを環境に適用し、報酬を取得\n",
        "            state, reward, done, _ = self.env.step(action)\n",
        "            rewards[step] = reward\n",
        "\n",
        "            if done or step + 1 >= n_steps:\n",
        "                curr_rewards = rewards[start_step:(step + 1)]\n",
        "                R = 0\n",
        "                for i in range(curr_rewards.numel() -1, -1, -1):\n",
        "                    R = curr_rewards[i] + args.gamma * R\n",
        "                    curr_rewards[i] = R\n",
        "                state = self.env.reset()\n",
        "                if start_step == 0:\n",
        "                    ep_reward = min(ep_reward, step - start_step + 1)\n",
        "                start_step = step + 1\n",
        "\n",
        "        return [rewards, ep_reward]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-wdXUUC-tBH"
      },
      "source": [
        "`Agent` のコンストラクターも 引数`batch`を取ります。これはアクションの確率値がバッチ処理されるかを制御するのに使用されます。\n",
        "\n",
        "バッチモードの場合、`saved_log_probs` はテンソルのリストを含んでおり、さらに各テンソルは、あるステップ内のすべてのオブザーバーから取得したアクションの確率値を含んでいます。\n",
        "\n",
        "一方でバッチ化を行わない場合、`saved_log_probs` は、keyがオブザーバーid、valueが対象のオブザーバーにおけるアクションの確率値である辞書型のオブジェクトになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZMwfOnM-tBH"
      },
      "source": [
        "import threading\n",
        "from torch.distributed.rpc import RRef\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, world_size, batch=True):\n",
        "        self.ob_rrefs = []\n",
        "        self.agent_rref = RRef(self)\n",
        "        self.rewards = {}\n",
        "        self.policy = Policy(batch).cuda()\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n",
        "        self.running_reward = 0\n",
        "\n",
        "        for ob_rank in range(1, world_size):\n",
        "            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n",
        "            self.ob_rrefs.append(rpc.remote(ob_info, Observer, args=(batch,)))\n",
        "            self.rewards[ob_info.id] = []\n",
        "\n",
        "        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n",
        "        self.batch = batch\n",
        "        self.saved_log_probs = [] if batch else {k:[] for k in range(len(self.ob_rrefs))}\n",
        "        self.future_actions = torch.futures.Future()\n",
        "        self.lock = threading.Lock()\n",
        "        self.pending_states = len(self.ob_rrefs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uoDmCwc-tBI"
      },
      "source": [
        "バッチ化を行わない `select_action` は、単にPolicy経由で状態を実行し、アクションの確率値を保存して、すぐにオブザーバーにアクションを返します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIoqj8Qk-tBI"
      },
      "source": [
        "from torch.distributions import Categorical\n",
        "\n",
        "class Agent:\n",
        "    # ...\n",
        "\n",
        "    @staticmethod\n",
        "    def select_action(agent_rref, ob_id, state):\n",
        "        self = agent_rref.local_value()\n",
        "        probs = self.policy(state.cuda())\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        self.saved_log_probs[ob_id].append(m.log_prob(action))\n",
        "        return action.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7qGOLcw-tBI"
      },
      "source": [
        "一方でバッチ処理を行うselect_action_batchの場合、状態は、オブザーバーidを行idとして使い、2次元テンソルである self.states に格納されます。\n",
        "\n",
        "これは、オブザーバーidによってインデックスされた特定の行に存在する、バッチ生成された `self.future_actions` Future オブジェクトにコールバック関数をインストールすることで Future をつなぎ合わせます。\n",
        "\n",
        "そして、最後に到着するオブザーバーは、ポリシーを通してバッチ化されたすべての状態を一度に実行し、同時に self.future_actions を設定します。\n",
        "\n",
        "これが行われた際、self.future_actions上にインストールされたすべてのコールバック関数のトリガーが発動し、コールバック関数の返り値がつなぎ合わされたFutureオブジェ\n",
        "クトを配置するために使用されます。\n",
        "\n",
        "それに伴い、Agentに対して準備するように通知し、他のオブザーバーからの過去のRPCリクエストのすべてに対してレスポンスを返します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IESSNBN9-tBI"
      },
      "source": [
        "class Agent:\n",
        "    # ...\n",
        "\n",
        "    @staticmethod\n",
        "    @rpc.functions.async_execution\n",
        "    def select_action_batch(agent_rref, ob_id, state):\n",
        "        self = agent_rref.local_value()\n",
        "        self.states[ob_id].copy_(state)\n",
        "        future_action = self.future_actions.then(\n",
        "            lambda future_actions: future_actions.wait()[ob_id].item()\n",
        "        )\n",
        "\n",
        "        with self.lock:\n",
        "            self.pending_states -= 1\n",
        "            if self.pending_states == 0:\n",
        "                self.pending_states = len(self.ob_rrefs)\n",
        "                probs = self.policy(self.states.cuda())\n",
        "                m = Categorical(probs)\n",
        "                actions = m.sample()\n",
        "                self.saved_log_probs.append(m.log_prob(actions).t()[0])\n",
        "                future_actions = self.future_actions\n",
        "                self.future_actions = torch.futures.Future()\n",
        "                future_actions.set_result(actions.cpu())\n",
        "        return future_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAvV7reJ-tBI"
      },
      "source": [
        "それでは、異なるRPC関数がどのように結合されるのか、定義してみましょう。\n",
        "\n",
        "`Agent` は、各エピソードの実行を制御します。\n",
        "\n",
        "初めに `rpc_async` を使用し、全オブザーバー上でエピソードを開始し、オブザーバーの報酬を含んだ返り値であるfutureをブロックします。\n",
        "\n",
        "なお、下記のコードでは、RRefを補助する `ob_rref.rpc_async()` を使用し、ob_rref RRefを所有しているワーカー上で、与えられた引数と共に `run_episode` 関数を起動している点に留意してください。\n",
        "\n",
        "そして、保存されたアクションの確率値と返されたオブザーバーの報酬を所定のデータフォーマットに変換し、訓練ステップを起動します。\n",
        "\n",
        "最後に、すべての状態をリセットし、現在のエピソードの報酬を返します。\n",
        "\n",
        "この関数は、あるエピソードを実行する際のエントリーポイントになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6ip-uIg-tBJ"
      },
      "source": [
        "class Agent:\n",
        "    # ...\n",
        "\n",
        "    def run_episode(self, n_steps=0):\n",
        "        futs = []\n",
        "        for ob_rref in self.ob_rrefs:\n",
        "            # 非同期RPCを作り、すべてのオブザーバー上でエピソードを開始\n",
        "            futs.append(ob_rref.rpc_async().run_episode(self.agent_rref, n_steps))\n",
        "\n",
        "        # すべてのオブザーバーがこのエピソードを完了するまで待機\n",
        "        wait until all obervers have finished this episode\n",
        "        rets = torch.futures.wait_all(futs)\n",
        "        rewards = torch.stack([ret[0] for ret in rets]).cuda().t()\n",
        "        ep_rewards = sum([ret[1] for ret in rets]) / len(rets)\n",
        "\n",
        "        # 保存された確率値を一つのテンソルにstack\n",
        "        if self.batch:\n",
        "            probs = torch.stack(self.saved_log_probs)\n",
        "        else:\n",
        "            probs = [torch.stack(self.saved_log_probs[i]) for i in range(len(rets))]\n",
        "            probs = torch.stack(probs)\n",
        "\n",
        "        policy_loss = -probs * rewards / len(rets)\n",
        "        policy_loss.sum().backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # 変数のリセット\n",
        "        self.saved_log_probs = [] if self.batch else {k:[] for k in range(len(self.ob_rrefs))}\n",
        "        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n",
        "\n",
        "        # 実行中の報酬を計算\n",
        "        self.running_reward = 0.5 * ep_rewards + 0.5 * self.running_reward\n",
        "        return ep_rewards, self.running_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0EVWtKT-tBJ"
      },
      "source": [
        "残りのコードは、他のRPCのチュートリアルと同様の、起動とロギングの通常の処理です。\n",
        "\n",
        "本チュートリアルでは、すべてのオブザーバーはエージェントからの指令を受動的に待機しています。\n",
        "\n",
        "実装の全内容については、[サンプルコード](https://github.com/pytorch/examples/tree/master/distributed/rpc)のリポジトリを参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oylaz_eW-tBJ"
      },
      "source": [
        "def run_worker(rank, world_size, n_episode, batch, print_log=True):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    if rank == 0:\n",
        "        # ランク0はエージェント\n",
        "        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n",
        "\n",
        "        agent = Agent(world_size, batch)\n",
        "        for i_episode in range(n_episode):\n",
        "            last_reward, running_reward = agent.run_episode(n_steps=NUM_STEPS)\n",
        "\n",
        "            if print_log:\n",
        "                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                    i_episode, last_reward, running_reward))\n",
        "    else:\n",
        "        # その他のランクはオブザーバー\n",
        "        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n",
        "        # オブザーバーは、エージェントからの支持を受動的に待機\n",
        "    rpc.shutdown()\n",
        "\n",
        "\n",
        "def main():\n",
        "    for world_size in range(2, 12):\n",
        "        delays = []\n",
        "        for batch in [True, False]:\n",
        "            tik = time.time()\n",
        "            mp.spawn(\n",
        "                run_worker,\n",
        "                args=(world_size, args.num_episode, batch),\n",
        "                nprocs=world_size,\n",
        "                join=True\n",
        "            )\n",
        "            tok = time.time()\n",
        "            delays.append(tok - tik)\n",
        "\n",
        "        print(f\"{world_size}, {delays[0]}, {delays[1]}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BakXrye7-tBJ"
      },
      "source": [
        "バッチRPCは、アクションの推論をより少ないCUDAの操作に統合する上で役立ち、その結果、オーバーヘッドを減らすことができます。\n",
        "\n",
        "上記の `main` 関数は、1から10までの異なる数のオブザーバーを使用し、バッチモードと非バッチモードの両方で同一のコードを実行します。\n",
        "\n",
        "下の図は、デフォルトの引数の値を使用し、異なるワールドサイズ（実行されるobserverの数）にしたときの実行時間をプロットしたものです。\n",
        "\n",
        "バッチ処理は訓練の高速化に役立つ、という期待通りの結果が確認できます。\n",
        "<img src=\"https://pytorch.org/tutorials/_images/batch.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLCn7z8f-tBK"
      },
      "source": [
        "## さらに学習するための資料集\n",
        "\n",
        "- [バッチ更新パラメーターサーバーのソースコード](https://github.com/pytorch/examples/blob/master/distributed/rpc/batch/parameter_server.py)\n",
        "- [バッチ処理カートポールソルバー](https://github.com/pytorch/examples/blob/master/distributed/rpc/batch/reinforce.py)\n",
        "- [分散自動微分](https://pytorch.org/docs/master/rpc.html#distributed-autograd-framework)\n",
        "- [分散パイプライン並列化](https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html)（日本語版6_7）"
      ]
    }
  ]
}